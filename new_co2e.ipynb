{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e097001-ed07-4679-b22b-374d0c07bf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 8012383\n",
      "\n",
      "List of columns:\n",
      "carrier\n",
      "carrier_cd_icao\n",
      "fltno\n",
      "fltsuffx\n",
      "depapt\n",
      "dep_port_cd_icao\n",
      "depterm\n",
      "depcity\n",
      "depctry\n",
      "arrapt\n",
      "arr_port_cd_icao\n",
      "arrterm\n",
      "arrcity\n",
      "arrctry\n",
      "deptim\n",
      "arrtim\n",
      "arrday\n",
      "elptim\n",
      "days\n",
      "efffrom\n",
      "effto\n",
      "stops\n",
      "genacft\n",
      "inpacft\n",
      "equipment_cd_icao\n",
      "acftchange\n",
      "service\n",
      "px\n",
      "seats\n",
      "seatsfst\n",
      "seatsbus\n",
      "seatspremeco\n",
      "seatseco\n",
      "meals\n",
      "frtclass\n",
      "tons\n",
      "comm10_50\n",
      "operating\n",
      "ghost\n",
      "dupcarfl\n",
      "sad\n",
      "sad_name\n",
      "acft_owner\n",
      "restrict\n",
      "domint\n",
      "routing\n",
      "longest\n",
      "intapt\n",
      "distance\n",
      "govt_app\n",
      "infltservice\n",
      "secureflt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = r\"D:\\work\\WDF_032025.csv\"\n",
    "\n",
    "# Define chunk size (adjust based on your system's memory)\n",
    "chunk_size = 10000  # Number of rows per chunk\n",
    "\n",
    "# Initialize variables\n",
    "total_rows = 0\n",
    "columns = None\n",
    "\n",
    "# Read the dataset in chunks\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "    # Add the number of rows in this chunk to the total\n",
    "    total_rows += len(chunk)\n",
    "    \n",
    "    # Store the columns from the first chunk (they'll be the same for all chunks)\n",
    "    if columns is None:\n",
    "        columns = chunk.columns.tolist()\n",
    "\n",
    "# Print the total number of rows\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "\n",
    "# Print the list of columns\n",
    "print(\"\\nList of columns:\")\n",
    "for col in columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6283a0f-3901-4f60-a22b-ba16f1243dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns dropped successfully. Updated dataset saved to D:\\work\\WDF_032025_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "input_file = r\"D:\\work\\WDF_032025.csv\"\n",
    "output_file = r\"D:\\work\\WDF_032025_cleaned.csv\"\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 10000\n",
    "\n",
    "# List of columns to drop\n",
    "columns_to_drop = [\n",
    "    'comm10_50',\n",
    "    'ghost',\n",
    "    'dupcarfl',\n",
    "    'sad',\n",
    "    'sad_name',\n",
    "    'govt_app',\n",
    "    'secureflt',\n",
    "    'tons'\n",
    "]\n",
    "\n",
    "# Process the dataset in chunks\n",
    "first_chunk = True\n",
    "for chunk in pd.read_csv(input_file, chunksize=chunk_size):\n",
    "    # Drop the specified columns\n",
    "    chunk = chunk.drop(columns=columns_to_drop, errors='ignore')\n",
    "    \n",
    "    # Save to output file (append mode)\n",
    "    if first_chunk:\n",
    "        chunk.to_csv(output_file, index=False, mode='w')\n",
    "        first_chunk = False\n",
    "    else:\n",
    "        chunk.to_csv(output_file, index=False, mode='a', header=False)\n",
    "\n",
    "print(f\"Columns dropped successfully. Updated dataset saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6de9aee-4f2f-40e3-9ef2-198c4c312690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance converted from miles to kilometers and column renamed to 'distance_in_km'. Updated dataset saved to D:\\work\\WDF_032025_cleaned_km.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "input_file = r\"D:\\work\\WDF_032025_cleaned.csv\"\n",
    "output_file = r\"D:\\work\\WDF_032025_cleaned_km.csv\"\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 10000\n",
    "\n",
    "# Conversion factor: 1 mile = 1.60934 km\n",
    "MILES_TO_KM = 1.60934\n",
    "\n",
    "# Process the dataset in chunks\n",
    "first_chunk = True\n",
    "for chunk in pd.read_csv(input_file, chunksize=chunk_size):\n",
    "    # Convert distance from miles to kilometers\n",
    "    chunk['distance'] = chunk['distance'] * MILES_TO_KM\n",
    "    \n",
    "    # Rename the 'distance' column to 'distance_in_km'\n",
    "    chunk = chunk.rename(columns={'distance': 'distance_in_km'})\n",
    "    \n",
    "    # Save to output file (append mode)\n",
    "    if first_chunk:\n",
    "        chunk.to_csv(output_file, index=False, mode='w')\n",
    "        first_chunk = False\n",
    "    else:\n",
    "        chunk.to_csv(output_file, index=False, mode='a', header=False)\n",
    "\n",
    "print(f\"Distance converted from miles to kilometers and column renamed to 'distance_in_km'. Updated dataset saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9b10641-1e54-41c0-94a9-1030b28cefc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data for columns 'distance_in_km', 'efffrom', and 'effto':\n",
      "   distance_in_km   efffrom     effto\n",
      "0        75.63898  20250319  20250319\n",
      "1        75.63898  20250326  20250326\n",
      "2        75.63898  20250402  20250402\n",
      "3        75.63898  20250409  20250409\n",
      "4        75.63898  20250416  20250416\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = r\"D:\\work\\WDF_032025_cleaned_km.csv\"\n",
    "\n",
    "# Read only the first 5 rows of the dataset\n",
    "df = pd.read_csv(file_path, nrows=5)\n",
    "\n",
    "# Select the columns of interest\n",
    "sample_data = df[['distance_in_km', 'efffrom', 'effto']]\n",
    "\n",
    "# Display the sample data\n",
    "print(\"Sample data for columns 'distance_in_km', 'efffrom', and 'effto':\")\n",
    "print(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d129dac0-dc56-4c4b-93b9-313b69cca9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'efffrom' and 'effto' columns converted to proper date format. Updated dataset saved to D:\\work\\WDF_032025_cleaned_km_dates.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "input_file = r\"D:\\work\\WDF_032025_cleaned_km.csv\"\n",
    "output_file = r\"D:\\work\\WDF_032025_cleaned_km_dates.csv\"\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 10000\n",
    "\n",
    "# Process the dataset in chunks\n",
    "first_chunk = True\n",
    "for chunk in pd.read_csv(input_file, chunksize=chunk_size):\n",
    "    # Convert 'efffrom' and 'effto' to proper date format\n",
    "    chunk['efffrom'] = pd.to_datetime(chunk['efffrom'], format='%Y%m%d', errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "    chunk['effto'] = pd.to_datetime(chunk['effto'], format='%Y%m%d', errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Save to output file (append mode)\n",
    "    if first_chunk:\n",
    "        chunk.to_csv(output_file, index=False, mode='w')\n",
    "        first_chunk = False\n",
    "    else:\n",
    "        chunk.to_csv(output_file, index=False, mode='a', header=False)\n",
    "\n",
    "print(f\"'efffrom' and 'effto' columns converted to proper date format. Updated dataset saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "295acb01-af33-4b9c-a914-2d660867cbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data (first 5 rows) of the dataset:\n",
      "  carrier carrier_cd_icao  fltno  fltsuffx depapt dep_port_cd_icao  depterm  \\\n",
      "0      VT             VTA    979       NaN    AAA             NTGA      NaN   \n",
      "1      VT             VTA    959       NaN    AAA             NTGA      NaN   \n",
      "2      VT             VTA    959       NaN    AAA             NTGA      NaN   \n",
      "3      VT             VTA    959       NaN    AAA             NTGA      NaN   \n",
      "4      VT             VTA    979       NaN    AAA             NTGA      NaN   \n",
      "\n",
      "  depcity depctry arrapt  ... frtclass  operating acft_owner restrict  domint  \\\n",
      "0     AAA      PF    FAC  ...      NaN        NaN        NaN      NaN      DD   \n",
      "1     AAA      PF    FAC  ...      NaN        NaN        NaN      NaN      DD   \n",
      "2     AAA      PF    FAC  ...      NaN        NaN        NaN      NaN      DD   \n",
      "3     AAA      PF    FAC  ...      NaN        NaN        NaN      NaN      DD   \n",
      "4     AAA      PF    FAC  ...      NaN        NaN        NaN      NaN      DD   \n",
      "\n",
      "           routing  longest  intapt  distance_in_km infltservice  \n",
      "0  PPTAAAFACRRRPPT      NaN     NaN        75.63898            9  \n",
      "1     PPTAAAFACPPT      NaN     NaN        75.63898            9  \n",
      "2     PPTAAAFACPPT      NaN     NaN        75.63898            9  \n",
      "3     PPTAAAFACPPT      NaN     NaN        75.63898            9  \n",
      "4  PPTAAAFACRRRPPT      NaN     NaN        75.63898            9  \n",
      "\n",
      "[5 rows x 44 columns]\n",
      "\n",
      "List of columns in the dataset:\n",
      "['carrier', 'carrier_cd_icao', 'fltno', 'fltsuffx', 'depapt', 'dep_port_cd_icao', 'depterm', 'depcity', 'depctry', 'arrapt', 'arr_port_cd_icao', 'arrterm', 'arrcity', 'arrctry', 'deptim', 'arrtim', 'arrday', 'elptim', 'days', 'efffrom', 'effto', 'stops', 'genacft', 'inpacft', 'equipment_cd_icao', 'acftchange', 'service', 'px', 'seats', 'seatsfst', 'seatsbus', 'seatspremeco', 'seatseco', 'meals', 'frtclass', 'operating', 'acft_owner', 'restrict', 'domint', 'routing', 'longest', 'intapt', 'distance_in_km', 'infltservice']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = r\"D:\\work\\WDF_032025_cleaned_km_dates.csv\"\n",
    "\n",
    "# Read only the first 5 rows of the dataset\n",
    "df = pd.read_csv(file_path, nrows=5)\n",
    "\n",
    "# Display the sample data (all columns)\n",
    "print(\"Sample data (first 5 rows) of the dataset:\")\n",
    "print(df)\n",
    "\n",
    "# Optionally, print the list of columns to confirm\n",
    "print(\"\\nList of columns in the dataset:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a00fee0-fe37-4f86-88bd-719d29e349e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data for columns 'distance_in_km', 'efffrom', and 'effto':\n",
      "   distance_in_km     efffrom       effto\n",
      "0        75.63898  2025-03-19  2025-03-19\n",
      "1        75.63898  2025-03-26  2025-03-26\n",
      "2        75.63898  2025-04-02  2025-04-02\n",
      "3        75.63898  2025-04-09  2025-04-09\n",
      "4        75.63898  2025-04-16  2025-04-16\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = r\"D:\\work\\WDF_032025_cleaned_km_dates.csv\"\n",
    "\n",
    "# Read only the first 5 rows of the dataset\n",
    "df = pd.read_csv(file_path, nrows=5)\n",
    "\n",
    "# Select the columns of interest\n",
    "sample_data = df[['distance_in_km', 'efffrom', 'effto']]\n",
    "\n",
    "# Display the sample data\n",
    "print(\"Sample data for columns 'distance_in_km', 'efffrom', and 'effto':\")\n",
    "print(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a543ea35-3160-456f-843c-1e1d47daa70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data for key columns:\n",
      "  equipment_cd_icao  seatsfst  seatsbus  seatspremeco  seatseco  stops\n",
      "0              AT72         0         0             0        58      0\n",
      "1              AT72         0         4             0        52      0\n",
      "2              AT72         0         0             0        56      0\n",
      "3              AT72         0         4             0        52      0\n",
      "4              AT72         0         4             0        60      0\n",
      "\n",
      "Data Quality Summary:\n",
      "Total rows: 8012383\n",
      "Non-stop flights (stops == 0): 7098749\n",
      "Rows with missing equipment_cd_icao: 1127928\n",
      "Rows with missing seat data (all seat columns 0 or NaN): 51520\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = r\"D:\\work\\WDF_032025_cleaned_km_dates.csv\"\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 10000\n",
    "\n",
    "# Key columns to check\n",
    "key_columns = ['equipment_cd_icao', 'seatsfst', 'seatsbus', 'seatspremeco', 'seatseco', 'stops']\n",
    "\n",
    "# Initialize variables\n",
    "sample_data = None\n",
    "total_rows = 0\n",
    "non_stop_rows = 0\n",
    "missing_equipment = 0\n",
    "missing_seats = 0\n",
    "\n",
    "# Process the dataset in chunks\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "    # Get sample data (first 5 rows) if not already collected\n",
    "    if sample_data is None:\n",
    "        sample_data = chunk[key_columns].head(5)\n",
    "    \n",
    "    # Count total rows\n",
    "    total_rows += len(chunk)\n",
    "    \n",
    "    # Count non-stop flights\n",
    "    non_stop_rows += len(chunk[chunk['stops'] == 0])\n",
    "    \n",
    "    # Count rows with missing equipment_cd_icao\n",
    "    missing_equipment += chunk['equipment_cd_icao'].isna().sum()\n",
    "    \n",
    "    # Count rows with missing seat data (all seat columns are 0 or NaN)\n",
    "    missing_seats += len(chunk[\n",
    "        (chunk['seatsfst'].isna() | (chunk['seatsfst'] == 0)) &\n",
    "        (chunk['seatsbus'].isna() | (chunk['seatsbus'] == 0)) &\n",
    "        (chunk['seatspremeco'].isna() | (chunk['seatspremeco'] == 0)) &\n",
    "        (chunk['seatseco'].isna() | (chunk['seatseco'] == 0))\n",
    "    ])\n",
    "\n",
    "# Display sample data for key columns\n",
    "print(\"Sample data for key columns:\")\n",
    "print(sample_data)\n",
    "\n",
    "# Display data quality summary\n",
    "print(\"\\nData Quality Summary:\")\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Non-stop flights (stops == 0): {non_stop_rows}\")\n",
    "print(f\"Rows with missing equipment_cd_icao: {missing_equipment}\")\n",
    "print(f\"Rows with missing seat data (all seat columns 0 or NaN): {missing_seats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86e323d1-b0dd-48bf-adab-d481e970ec88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cleaned: Default aircraft set for missing equipment_cd_icao, invalid seat rows dropped. Updated dataset saved to D:\\work\\WDF_032025_cleaned_final.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "input_file = r\"D:\\work\\WDF_032025_cleaned_km_dates.csv\"\n",
    "output_file = r\"D:\\work\\WDF_032025_cleaned_final.csv\"\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 10000\n",
    "\n",
    "# Process the dataset in chunks\n",
    "first_chunk = True\n",
    "for chunk in pd.read_csv(input_file, chunksize=chunk_size):\n",
    "    # Set default aircraft type 'A320' for missing equipment_cd_icao\n",
    "    chunk['equipment_cd_icao'] = chunk['equipment_cd_icao'].fillna('A320')\n",
    "    \n",
    "    # Drop rows where all seat columns are 0 or NaN\n",
    "    chunk = chunk[\n",
    "        ~(\n",
    "            (chunk['seatsfst'].isna() | (chunk['seatsfst'] == 0)) &\n",
    "            (chunk['seatsbus'].isna() | (chunk['seatsbus'] == 0)) &\n",
    "            (chunk['seatspremeco'].isna() | (chunk['seatspremeco'] == 0)) &\n",
    "            (chunk['seatseco'].isna() | (chunk['seatseco'] == 0))\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Save to output file (append mode)\n",
    "    if first_chunk:\n",
    "        chunk.to_csv(output_file, index=False, mode='w')\n",
    "        first_chunk = False\n",
    "    else:\n",
    "        chunk.to_csv(output_file, index=False, mode='a', header=False)\n",
    "\n",
    "print(f\"Dataset cleaned: Default aircraft set for missing equipment_cd_icao, invalid seat rows dropped. Updated dataset saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83489bb9-0142-4354-9e45-964048a8c27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data for key columns (cleaned dataset):\n",
      "  equipment_cd_icao  seatsfst  seatsbus  seatspremeco  seatseco  stops\n",
      "0              AT72         0         0             0        58      0\n",
      "1              AT72         0         4             0        52      0\n",
      "2              AT72         0         0             0        56      0\n",
      "3              AT72         0         4             0        52      0\n",
      "4              AT72         0         4             0        60      0\n",
      "\n",
      "Data Quality Summary (cleaned dataset):\n",
      "Total rows: 7960863\n",
      "Non-stop flights (stops == 0): 7052218\n",
      "Rows with missing equipment_cd_icao: 0\n",
      "Rows with missing seat data (all seat columns 0 or NaN): 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = r\"D:\\work\\WDF_032025_cleaned_final.csv\"\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 10000\n",
    "\n",
    "# Key columns to check\n",
    "key_columns = ['equipment_cd_icao', 'seatsfst', 'seatsbus', 'seatspremeco', 'seatseco', 'stops']\n",
    "\n",
    "# Initialize variables\n",
    "sample_data = None\n",
    "total_rows = 0\n",
    "non_stop_rows = 0\n",
    "missing_equipment = 0\n",
    "missing_seats = 0\n",
    "\n",
    "# Process the dataset in chunks\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "    # Get sample data (first 5 rows) if not already collected\n",
    "    if sample_data is None:\n",
    "        sample_data = chunk[key_columns].head(5)\n",
    "    \n",
    "    # Count total rows\n",
    "    total_rows += len(chunk)\n",
    "    \n",
    "    # Count non-stop flights\n",
    "    non_stop_rows += len(chunk[chunk['stops'] == 0])\n",
    "    \n",
    "    # Count rows with missing equipment_cd_icao\n",
    "    missing_equipment += chunk['equipment_cd_icao'].isna().sum()\n",
    "    \n",
    "    # Count rows with missing seat data (all seat columns 0 or NaN)\n",
    "    missing_seats += len(chunk[\n",
    "        (chunk['seatsfst'].isna() | (chunk['seatsfst'] == 0)) &\n",
    "        (chunk['seatsbus'].isna() | (chunk['seatsbus'] == 0)) &\n",
    "        (chunk['seatspremeco'].isna() | (chunk['seatspremeco'] == 0)) &\n",
    "        (chunk['seatseco'].isna() | (chunk['seatseco'] == 0))\n",
    "    ])\n",
    "\n",
    "# Display sample data for key columns\n",
    "print(\"Sample data for key columns (cleaned dataset):\")\n",
    "print(sample_data)\n",
    "\n",
    "# Display data quality summary\n",
    "print(\"\\nData Quality Summary (cleaned dataset):\")\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Non-stop flights (stops == 0): {non_stop_rows}\")\n",
    "print(f\"Rows with missing equipment_cd_icao: {missing_equipment}\")\n",
    "print(f\"Rows with missing seat data (all seat columns 0 or NaN): {missing_seats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6600a74c-d598-4039-b976-16784e1ad895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique equipment_cd_icao values in the dataset:\n",
      "['*', 'A109', 'A139', 'A148', 'A19N', 'A20N', 'A21N', 'A306', 'A30B', 'A318', 'A319', 'A320', 'A321', 'A332', 'A333', 'A338', 'A339', 'A342', 'A343', 'A346', 'A359', 'A35K', 'A388', 'AJ27', 'AN24', 'AN26', 'AN38', 'AS50', 'AT43', 'AT72', 'B06', 'B190', 'B37M', 'B38M', 'B39M', 'B462', 'B463', 'B712', 'B732', 'B733', 'B734', 'B735', 'B736', 'B737', 'B738', 'B739', 'B744', 'B748', 'B752', 'B753', 'B762', 'B763', 'B764', 'B772', 'B773', 'B77L', 'B77W', 'B788', 'B789', 'B78X', 'BCS1', 'BCS3', 'BN2P', 'C208', 'CL30', 'CRJ1', 'CRJ2', 'CRJ7', 'CRJ9', 'CRJX', 'D228', 'D328', 'DA42', 'DA62', 'DH3T', 'DH8A', 'DH8B', 'DH8C', 'DH8D', 'DHC2', 'DHC4', 'DHC6', 'DHC7', 'E110', 'E120', 'E135', 'E145', 'E170', 'E190', 'E195', 'E75L', 'EC30', 'EC55', 'F100', 'F50', 'F70', 'J328', 'JS31', 'JS32', 'JS41', 'L410', 'MD82', 'MD83', 'MI8', 'P212', 'PC12', 'RJ85', 'S76', 'SB20', 'SF34', 'SU95', 'SW4', 'T154', 'T204', 'TRIS', 'YK40', 'YK42']\n",
      "\n",
      "Total number of unique aircraft types: 117\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = r\"D:\\work\\WDF_032025_cleaned_final.csv\"\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 10000\n",
    "\n",
    "# Initialize a set to store unique equipment_cd_icao values\n",
    "unique_aircraft = set()\n",
    "\n",
    "# Process the dataset in chunks\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "    # Add unique equipment_cd_icao values to the set\n",
    "    unique_aircraft.update(chunk['equipment_cd_icao'].dropna().unique())\n",
    "\n",
    "# Convert the set to a sorted list for better readability\n",
    "unique_aircraft_list = sorted(unique_aircraft)\n",
    "\n",
    "# Display the unique aircraft types\n",
    "print(\"Unique equipment_cd_icao values in the dataset:\")\n",
    "print(unique_aircraft_list)\n",
    "print(f\"\\nTotal number of unique aircraft types: {len(unique_aircraft_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b4d3cf0-278a-4f1c-84b7-bbd58c45d94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CO2e calculations completed for all records. Results saved to D:\\work\\WDF_032025_CO2e.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "input_file = r\"D:\\work\\WDF_032025_cleaned_final.csv\"\n",
    "output_file = r\"D:\\work\\WDF_032025_CO2e.csv\"\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 10000\n",
    "\n",
    "# Define constants for CO2e calculation\n",
    "BASE_EMISSION_FACTORS = {\n",
    "    \"short_haul\": 0.12,  # < 1500 km\n",
    "    \"medium_haul\": 0.105,  # 1500–4000 km\n",
    "    \"long_haul\": 0.085  # > 4000 km\n",
    "}\n",
    "\n",
    "# Updated AIRCRAFT_MULTIPLIERS dictionary\n",
    "AIRCRAFT_MULTIPLIERS = {\n",
    "    \"*\": 1.0,\n",
    "    \"A109\": 1.5, \"A139\": 1.5, \"AS50\": 1.5, \"B06\": 1.5, \"EC30\": 1.5, \"EC55\": 1.5, \"MI8\": 1.5, \"S76\": 1.5,\n",
    "    \"AT43\": 1.2, \"AT72\": 1.2, \"B190\": 1.2, \"BN2P\": 1.2, \"C208\": 1.2, \"D228\": 1.2, \"D328\": 1.2, \"DA42\": 1.2,\n",
    "    \"DA62\": 1.2, \"DH8A\": 1.2, \"DH8B\": 1.2, \"DH8C\": 1.2, \"DH8D\": 1.2, \"DHC2\": 1.2, \"DHC4\": 1.2, \"DHC6\": 1.2,\n",
    "    \"DHC7\": 1.2, \"E110\": 1.2, \"E120\": 1.2, \"E135\": 1.2, \"E145\": 1.2, \"E75L\": 1.2, \"F50\": 1.2, \"F70\": 1.2,\n",
    "    \"J328\": 1.2, \"JS31\": 1.2, \"JS32\": 1.2, \"JS41\": 1.2, \"L410\": 1.2, \"PC12\": 1.2, \"SB20\": 1.2, \"SF34\": 1.2,\n",
    "    \"SW4\": 1.2, \"TRIS\": 1.2,\n",
    "    \"CRJ1\": 1.15, \"CRJ2\": 1.15, \"CRJ7\": 1.15, \"CRJ9\": 1.15, \"CRJX\": 1.15, \"E170\": 1.15, \"E190\": 1.15,\n",
    "    \"E195\": 1.15, \"SU95\": 1.15,\n",
    "    \"A19N\": 1.0, \"A20N\": 1.0, \"A21N\": 1.0, \"A318\": 1.0, \"A319\": 1.05, \"A320\": 1.0, \"A321\": 0.95,\n",
    "    \"B37M\": 1.0, \"B38M\": 1.0, \"B39M\": 1.0, \"B712\": 1.0, \"B737\": 1.0, \"B738\": 1.0, \"B739\": 1.0,\n",
    "    \"B752\": 1.0, \"B753\": 1.0, \"BCS1\": 1.0, \"BCS3\": 1.0,\n",
    "    \"B732\": 1.05, \"B733\": 1.05, \"B734\": 1.05, \"B735\": 1.05, \"B736\": 1.05, \"MD82\": 1.05, \"MD83\": 1.05,\n",
    "    \"A332\": 0.90, \"A333\": 0.90, \"A338\": 0.90, \"A339\": 0.90, \"A359\": 0.90, \"A35K\": 0.90,\n",
    "    \"B764\": 0.90, \"B772\": 0.90, \"B773\": 0.90, \"B77L\": 0.90, \"B77W\": 0.95, \"B788\": 0.90, \"B789\": 0.90, \"B78X\": 0.90,\n",
    "    \"A306\": 1.1, \"A30B\": 1.1, \"A342\": 1.1, \"A343\": 1.1, \"A346\": 1.1, \"A388\": 1.1,\n",
    "    \"B744\": 1.1, \"B748\": 1.1, \"B762\": 1.1, \"B763\": 1.1,\n",
    "    \"AJ27\": 1.3, \"AN24\": 1.3, \"AN26\": 1.3, \"AN38\": 1.3, \"T154\": 1.3, \"T204\": 1.3, \"YK40\": 1.3, \"YK42\": 1.3,\n",
    "    \"CL30\": 1.4, \"P212\": 1.4,\n",
    "    \"B462\": 1.2, \"B463\": 1.2, \"F100\": 1.2, \"RJ85\": 1.2,\n",
    "    \"default\": 1.0\n",
    "}\n",
    "\n",
    "ADDITIONAL_FACTOR = 1.05\n",
    "\n",
    "# Function to determine flight category based on distance\n",
    "def get_flight_category(distance_km):\n",
    "    if distance_km < 1500:\n",
    "        return \"short_haul\"\n",
    "    elif 1500 <= distance_km <= 4000:\n",
    "        return \"medium_haul\"\n",
    "    else:\n",
    "        return \"long_haul\"\n",
    "\n",
    "# Function to calculate seat class multiplier\n",
    "def calculate_seat_class_multiplier(row):\n",
    "    total_seats = row['seatsfst'] + row['seatsbus'] + row['seatspremeco'] + row['seatseco']\n",
    "    if total_seats == 0:\n",
    "        return 1.0  # This should not happen as we dropped invalid rows\n",
    "    seat_class_multiplier = (\n",
    "        (row['seatsfst'] / total_seats) * 3.0 +\n",
    "        (row['seatsbus'] / total_seats) * 2.0 +\n",
    "        (row['seatspremeco'] / total_seats) * 1.5 +\n",
    "        (row['seatseco'] / total_seats) * 1.0\n",
    "    )\n",
    "    return seat_class_multiplier\n",
    "\n",
    "# Function to calculate CO2e per passenger\n",
    "def calculate_co2e_per_passenger(row):\n",
    "    # Step 1: Get distance\n",
    "    distance_km = row['distance_in_km']\n",
    "\n",
    "    # Step 2: Determine flight category and base emission factor\n",
    "    flight_category = get_flight_category(distance_km)\n",
    "    base_emission_factor = BASE_EMISSION_FACTORS[flight_category]\n",
    "\n",
    "    # Step 3: Get aircraft multiplier\n",
    "    aircraft_code = row['equipment_cd_icao']\n",
    "    aircraft_multiplier = AIRCRAFT_MULTIPLIERS.get(aircraft_code, AIRCRAFT_MULTIPLIERS[\"default\"])\n",
    "\n",
    "    # Step 4: Calculate seat class multiplier\n",
    "    seat_class_multiplier = calculate_seat_class_multiplier(row)\n",
    "\n",
    "    # Step 5: Calculate CO2e per passenger\n",
    "    co2e_per_passenger = (\n",
    "        distance_km *\n",
    "        base_emission_factor *\n",
    "        aircraft_multiplier *\n",
    "        seat_class_multiplier *\n",
    "        ADDITIONAL_FACTOR\n",
    "    )\n",
    "\n",
    "    return round(co2e_per_passenger, 2)\n",
    "\n",
    "# Process the dataset in chunks\n",
    "first_chunk = True\n",
    "for chunk in pd.read_csv(input_file, chunksize=chunk_size):\n",
    "    # Step 1: Calculate CO2e for each row\n",
    "    chunk['co2e_per_passenger_kg'] = chunk.apply(calculate_co2e_per_passenger, axis=1)\n",
    "\n",
    "    # Step 2: Select relevant columns for output\n",
    "    output_columns = [\n",
    "        'carrier', 'fltno', 'fltsuffx', 'depapt', 'arrapt', \n",
    "        'distance_in_km', 'equipment_cd_icao', 'seatsfst', 'seatsbus', \n",
    "        'seatspremeco', 'seatseco', 'stops', 'co2e_per_passenger_kg'\n",
    "    ]\n",
    "    chunk = chunk[output_columns]\n",
    "\n",
    "    # Step 3: Save to output file (append mode)\n",
    "    if first_chunk:\n",
    "        chunk.to_csv(output_file, index=False, mode='w')\n",
    "        first_chunk = False\n",
    "    else:\n",
    "        chunk.to_csv(output_file, index=False, mode='a', header=False)\n",
    "\n",
    "print(f\"CO2e calculations completed for all records. Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c384e6b-564c-45dc-a9e6-2fc7bcbcd9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample CO2e calculation results (first 5 rows):\n",
      "  carrier  fltno  fltsuffx depapt arrapt  distance_in_km equipment_cd_icao  \\\n",
      "0      VT    979       NaN    AAA    FAC        75.63898              AT72   \n",
      "1      VT    959       NaN    AAA    FAC        75.63898              AT72   \n",
      "2      VT    959       NaN    AAA    FAC        75.63898              AT72   \n",
      "3      VT    959       NaN    AAA    FAC        75.63898              AT72   \n",
      "4      VT    979       NaN    AAA    FAC        75.63898              AT72   \n",
      "\n",
      "   seatsfst  seatsbus  seatspremeco  seatseco  stops  co2e_per_passenger_kg  \n",
      "0         0         0             0        58      0                  11.44  \n",
      "1         0         4             0        52      0                  12.25  \n",
      "2         0         0             0        56      0                  11.44  \n",
      "3         0         4             0        52      0                  12.25  \n",
      "4         0         4             0        60      0                  12.15  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = r\"D:\\work\\WDF_032025_CO2e.csv\"\n",
    "\n",
    "# Read only the first 5 rows\n",
    "df = pd.read_csv(file_path, nrows=5)\n",
    "\n",
    "# Display the sample data\n",
    "print(\"Sample CO2e calculation results (first 5 rows):\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccedc641-d590-4432-afe9-0ee48dd5c5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\anaconda\\lib\\site-packages (3.1.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: et-xmlfile in c:\\anaconda\\lib\\site-packages (from openpyxl) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0fe6293f-a060-4d33-85fa-bd428f7d51b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in airlines_with_amenities.xlsx:\n",
      "['Airline Name', 'Plane Types', 'Amenities']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "seatguru_file = r\"C:\\Users\\amit.murmu\\Downloads\\airlines_with_amenities.xlsx\"\n",
    "\n",
    "# Load the Excel file\n",
    "seatguru_df = pd.read_excel(seatguru_file)\n",
    "\n",
    "# Print the column names\n",
    "print(\"Columns in airlines_with_amenities.xlsx:\")\n",
    "print(seatguru_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "305f0c54-ffa3-4d1b-90ad-f390b4237f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amenities column added. Updated dataset saved to D:\\work\\WDF_032025_CO2e_with_amenities.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "main_file = r\"D:\\work\\WDF_032025_CO2e.csv\"\n",
    "seatguru_file = r\"C:\\Users\\amit.murmu\\Downloads\\airlines_with_amenities.xlsx\"\n",
    "output_file = r\"D:\\work\\WDF_032025_CO2e_with_amenities.csv\"\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 10000\n",
    "\n",
    "# Step 1: Load SeatGuru data from Excel file\n",
    "seatguru_df = pd.read_excel(seatguru_file)\n",
    "\n",
    "# Step 2: Rename columns in SeatGuru data to match main dataset\n",
    "seatguru_df = seatguru_df.rename(columns={\n",
    "    'Airline Name': 'carrier',\n",
    "    'Plane Types': 'equipment_cd_icao',\n",
    "    'Amenities': 'amenities'\n",
    "})\n",
    "\n",
    "# Step 3: Create a merge key in SeatGuru data\n",
    "# Combine carrier and equipment_cd_icao to create a unique key\n",
    "seatguru_df['merge_key'] = seatguru_df['carrier'].astype(str) + '_' + \\\n",
    "                          seatguru_df['equipment_cd_icao'].astype(str)\n",
    "\n",
    "# Step 4: Process the main dataset in chunks and merge with SeatGuru data\n",
    "first_chunk = True\n",
    "for chunk in pd.read_csv(main_file, chunksize=chunk_size):\n",
    "    # Create the same merge key in the main dataset chunk\n",
    "    chunk['merge_key'] = chunk['carrier'].astype(str) + '_' + \\\n",
    "                        chunk['equipment_cd_icao'].astype(str)\n",
    "\n",
    "    # Merge with SeatGuru data (left join)\n",
    "    chunk = chunk.merge(seatguru_df[['merge_key', 'amenities']], \n",
    "                        on='merge_key', \n",
    "                        how='left')\n",
    "\n",
    "    # Drop the merge_key column as it's no longer needed\n",
    "    chunk = chunk.drop(columns=['merge_key'])\n",
    "\n",
    "    # Save to output file (append mode)\n",
    "    if first_chunk:\n",
    "        chunk.to_csv(output_file, index=False, mode='w')\n",
    "        first_chunk = False\n",
    "    else:\n",
    "        chunk.to_csv(output_file, index=False, mode='a', header=False)\n",
    "\n",
    "print(f\"Amenities column added. Updated dataset saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99a91d19-b468-4e1e-aec5-2573ecacb1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data with amenities (first 5 rows):\n",
      "  carrier  fltno fltsuffx depapt arrapt  distance_in_km equipment_cd_icao  \\\n",
      "0      VT    979      NaN    AAA    FAC        75.63898              AT72   \n",
      "1      VT    959      NaN    AAA    FAC        75.63898              AT72   \n",
      "2      VT    959      NaN    AAA    FAC        75.63898              AT72   \n",
      "3      VT    959      NaN    AAA    FAC        75.63898              AT72   \n",
      "4      VT    979      NaN    AAA    FAC        75.63898              AT72   \n",
      "\n",
      "   seatsfst  seatsbus  seatspremeco  seatseco  stops  co2e_per_passenger_kg  \\\n",
      "0         0         0             0        58      0                  11.44   \n",
      "1         0         4             0        52      0                  12.25   \n",
      "2         0         0             0        56      0                  11.44   \n",
      "3         0         4             0        52      0                  12.25   \n",
      "4         0         4             0        60      0                  12.15   \n",
      "\n",
      "   amenities  \n",
      "0        NaN  \n",
      "1        NaN  \n",
      "2        NaN  \n",
      "3        NaN  \n",
      "4        NaN  \n",
      "\n",
      "Data Quality Summary:\n",
      "Total rows: 7960863\n",
      "Rows with amenities data: 0\n",
      "Rows without amenities data: 7960863\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = r\"D:\\work\\WDF_032025_CO2e_with_amenities.csv\"\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 10000\n",
    "\n",
    "# Initialize variables\n",
    "sample_data = None\n",
    "total_rows = 0\n",
    "rows_with_amenities = 0\n",
    "\n",
    "# Process the dataset in chunks\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "    # Get sample data (first 5 rows) if not already collected\n",
    "    if sample_data is None:\n",
    "        sample_data = chunk.head(5)\n",
    "    \n",
    "    # Count total rows\n",
    "    total_rows += len(chunk)\n",
    "    \n",
    "    # Count rows with non-NaN amenities\n",
    "    rows_with_amenities += chunk['amenities'].notna().sum()\n",
    "\n",
    "# Display sample data\n",
    "print(\"Sample data with amenities (first 5 rows):\")\n",
    "print(sample_data)\n",
    "\n",
    "# Display data quality summary\n",
    "print(\"\\nData Quality Summary:\")\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Rows with amenities data: {rows_with_amenities}\")\n",
    "print(f\"Rows without amenities data: {total_rows - rows_with_amenities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f74ab9d2-5e25-4982-8b82-90ec5225a5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from SeatGuru dataset (first 5 rows):\n",
      "           carrier      equipment_cd_icao             amenities  \\\n",
      "0  Aegean Airlines      Airbus A319 (319)                  Food   \n",
      "1  Aegean Airlines      Airbus A320 (320)  Food, Tv, Headphones   \n",
      "2  Aegean Airlines      Airbus A321 (321)  Food, Tv, Headphones   \n",
      "3  Aegean Airlines       ATR 42-600 (ATR)                  Food   \n",
      "4  Aegean Airlines  Bombardier Q400 (DH4)                  Food   \n",
      "\n",
      "                               merge_key  \n",
      "0      Aegean Airlines_Airbus A319 (319)  \n",
      "1      Aegean Airlines_Airbus A320 (320)  \n",
      "2      Aegean Airlines_Airbus A321 (321)  \n",
      "3       Aegean Airlines_ATR 42-600 (ATR)  \n",
      "4  Aegean Airlines_Bombardier Q400 (DH4)  \n",
      "\n",
      "Sample from Main dataset (first 5 rows):\n",
      "  carrier equipment_cd_icao merge_key\n",
      "0      VT              AT72   VT_AT72\n",
      "1      VT              AT72   VT_AT72\n",
      "2      VT              AT72   VT_AT72\n",
      "3      VT              AT72   VT_AT72\n",
      "4      VT              AT72   VT_AT72\n",
      "\n",
      "Unique merge keys in SeatGuru dataset:\n",
      "['Aegean Airlines_Airbus A319 (319)' 'Aegean Airlines_Airbus A320 (320)'\n",
      " 'Aegean Airlines_Airbus A321 (321)' 'Aegean Airlines_ATR 42-600 (ATR)'\n",
      " 'Aegean Airlines_Bombardier Q400 (DH4)'\n",
      " 'Aer Lingus_Airbus A330-200 (332) Layout 1'\n",
      " 'Aer Lingus_Airbus A330-200 (332) Layout 2'\n",
      " 'Aer Lingus_Airbus A330-300 (333)' 'Aer Lingus_Airbus A320 (320)'\n",
      " 'Aer Lingus_Airbus A321 (321)']\n",
      "\n",
      "Unique merge keys in Main dataset (first 1000 rows):\n",
      "['VT_AT72' 'VT_AT43' 'JBW_F50' 'AH_B736' 'AH_AT72' 'AH_B738' 'AH_B737'\n",
      " '5O_B737' 'SF_DH8D' 'TK_B738']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "main_file = r\"D:\\work\\WDF_032025_CO2e.csv\"\n",
    "seatguru_file = r\"C:\\Users\\amit.murmu\\Downloads\\airlines_with_amenities.xlsx\"\n",
    "\n",
    "# Load SeatGuru data\n",
    "seatguru_df = pd.read_excel(seatguru_file)\n",
    "\n",
    "# Rename columns in SeatGuru data to match main dataset\n",
    "seatguru_df = seatguru_df.rename(columns={\n",
    "    'Airline Name': 'carrier',\n",
    "    'Plane Types': 'equipment_cd_icao',\n",
    "    'Amenities': 'amenities'\n",
    "})\n",
    "\n",
    "# Create merge key in SeatGuru data\n",
    "seatguru_df['merge_key'] = seatguru_df['carrier'].astype(str) + '_' + \\\n",
    "                          seatguru_df['equipment_cd_icao'].astype(str)\n",
    "\n",
    "# Load sample from main dataset (first 5 rows)\n",
    "main_df_sample = pd.read_csv(main_file, nrows=5)\n",
    "\n",
    "# Create merge key in main dataset sample\n",
    "main_df_sample['merge_key'] = main_df_sample['carrier'].astype(str) + '_' + \\\n",
    "                             main_df_sample['equipment_cd_icao'].astype(str)\n",
    "\n",
    "# Print sample data from both datasets\n",
    "print(\"Sample from SeatGuru dataset (first 5 rows):\")\n",
    "print(seatguru_df[['carrier', 'equipment_cd_icao', 'amenities', 'merge_key']].head())\n",
    "\n",
    "print(\"\\nSample from Main dataset (first 5 rows):\")\n",
    "print(main_df_sample[['carrier', 'equipment_cd_icao', 'merge_key']].head())\n",
    "\n",
    "# Check unique merge keys in both datasets\n",
    "print(\"\\nUnique merge keys in SeatGuru dataset:\")\n",
    "print(seatguru_df['merge_key'].unique()[:10])  # First 10 unique keys\n",
    "\n",
    "print(\"\\nUnique merge keys in Main dataset (first 1000 rows):\")\n",
    "main_df_larger_sample = pd.read_csv(main_file, nrows=1000)\n",
    "main_df_larger_sample['merge_key'] = main_df_larger_sample['carrier'].astype(str) + '_' + \\\n",
    "                                    main_df_larger_sample['equipment_cd_icao'].astype(str)\n",
    "print(main_df_larger_sample['merge_key'].unique()[:10])  # First 10 unique keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18e2c3b9-f041-4afb-b936-59016be6c10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All unique carriers in SeatGuru dataset:\n",
      "['ANA', 'Aegean Airlines', 'Aer Lingus', 'Aeroflot', 'Aerolineas Argentinas', 'Aeromexico', 'Air Arabia', 'Air Astana', 'Air Austral', 'Air Baltic', 'Air Belgium', 'Air Canada', 'Air Caraibes', 'Air China', 'Air Corsica', 'Air Dolomiti', 'Air Europa', 'Air France', 'Air India', 'Air India Express', 'Air Macau', 'Air Malta', 'Air Mauritius', 'Air Namibia', 'Air New Zealand', 'Air North', 'Air Seoul', 'Air Serbia', 'Air Tahiti Nui', 'Air Transat', 'Air Vanuatu', 'AirAsia', 'AirAsia X', 'Aircalin', 'Alaska Airlines', 'Alitalia', 'Allegiant', 'American Airlines', 'Asiana', 'Austrian', 'Azerbaijan Hava Yollary', 'Azores Airlines', 'Azul', 'Bamboo Airways', 'Bangkok Airways', 'British Airways', 'Brussels Airlines', 'CEBU Pacific Air', 'Caribbean Airlines', 'Cathay Dragon', 'Cathay Pacific', 'Cayman Airways', 'China Airlines', 'China Eastern', 'China Southern', 'Condor', 'Copa Airlines', 'Croatia Airlines', 'Czech Airlines', 'Delta', 'EL AL', 'EVA Air', 'Edelweiss Air', 'Egyptair', 'Emirates', 'Ethiopian Airlines', 'Etihad', 'Eurowings', 'Fiji Airways', 'Finnair', 'FlyOne', 'French bee', 'Frontier', 'Garuda Indonesia', 'Gol', 'Gulf Air', 'HK Express', 'Hainan Airlines', 'Hawaiian Airlines', 'Helvetic Airways', 'Hong Kong Airlines', 'Iberia', 'Icelandair', 'IndiGo Airlines', 'InterJet', 'Japan Airlines', 'Jeju Air', 'Jet2', 'JetBlue', 'Jetstar', 'Jin Air', 'KLM', 'Kenya Airways', 'Korean Air', 'Kulula', 'LATAM', 'LOT Polish Airlines', 'La Compagnie', 'Lion Airlines', 'Lufthansa', 'Luxair', 'Malaysia Airlines', 'Mango', 'Middle East Airlines', 'Nok Air', 'Nordwind Airlines', 'Norwegian Air International', 'Norwegian Air Shuttle', 'Norwegian Air Sweden', 'Norwegian Air UK', 'Oman Air', 'Pakistan International Airlines', 'Peach', 'Pegasus Airlines', 'Philippine Airlines', 'Porter', 'Qantas', 'Qatar Airways', 'Regional Express', 'Rossiya - Russian Airlines', 'Royal Air Maroc', 'Royal Brunei', 'Royal Jordanian', 'RwandAir', 'Ryanair', 'S7 Airlines', 'SAS', 'SWISS', 'Saudia', 'Scoot Airlines', 'Shanghai Airlines', 'Silkair', 'Silver', 'Singapore Airlines', 'Skylanes', 'South African Airways', 'Southwest', 'SpiceJet', 'Spirit', 'Spring Airlines', 'Spring Japan', 'SriLankan Airlines', 'Sun Country', 'Sunclass Airlines', 'Sunwing', 'Swoop', 'TAAG', 'TACA', 'TAP Portugal', 'THAI', 'TUI UK', 'TUIfly', 'Transavia Airlines', 'Tunis Air', 'Turkish Airlines', 'UTair Aviation', 'Ukraine International', 'United', 'Ural Airlines', 'Uzbekistan Airways', 'Vietnam Airlines', 'Virgin Atlantic', 'Virgin Australia', 'Vistara', 'Viva Aerobus', 'Volaris', 'Volotea', 'Vueling Airlines', 'WestJet', 'Wizzair', 'Xiamen Airlines', 'easyJet', 'flydubai', 'tigerair Australia']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "seatguru_file = r\"C:\\Users\\amit.murmu\\Downloads\\airlines_with_amenities.xlsx\"\n",
    "\n",
    "# Load SeatGuru data\n",
    "seatguru_df = pd.read_excel(seatguru_file)\n",
    "\n",
    "# Rename columns\n",
    "seatguru_df = seatguru_df.rename(columns={\n",
    "    'Airline Name': 'carrier',\n",
    "    'Plane Types': 'equipment_cd_icao',\n",
    "    'Amenities': 'amenities'\n",
    "})\n",
    "\n",
    "# Print all unique carriers\n",
    "print(\"All unique carriers in SeatGuru dataset:\")\n",
    "print(sorted(seatguru_df['carrier'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae8047ba-b782-4e75-b385-42d26fdc34ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amenities column added. Updated dataset saved to D:\\work\\WDF_032025_CO2e_with_amenities.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "main_file = r\"D:\\work\\WDF_032025_CO2e.csv\"\n",
    "seatguru_file = r\"C:\\Users\\amit.murmu\\Downloads\\airlines_with_amenities.xlsx\"\n",
    "output_file = r\"D:\\work\\WDF_032025_CO2e_with_amenities.csv\"\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 10000\n",
    "\n",
    "# Step 1: Load SeatGuru data\n",
    "seatguru_df = pd.read_excel(seatguru_file)\n",
    "\n",
    "# Rename columns in SeatGuru data\n",
    "seatguru_df = seatguru_df.rename(columns={\n",
    "    'Airline Name': 'carrier',\n",
    "    'Plane Types': 'equipment_cd_icao',\n",
    "    'Amenities': 'amenities'\n",
    "})\n",
    "\n",
    "# Step 2: Standardize equipment_cd_icao in SeatGuru dataset\n",
    "def extract_icao_code(plane_type):\n",
    "    # Remove extra spaces and convert to uppercase\n",
    "    plane_type = str(plane_type).strip().upper()\n",
    "    \n",
    "    # Common mappings for known plane types\n",
    "    plane_mapping = {\n",
    "        'AIRBUS A319 (319)': 'A319',\n",
    "        'AIRBUS A320 (320)': 'A320',\n",
    "        'AIRBUS A321 (321)': 'A321',\n",
    "        'ATR 42-600 (ATR)': 'AT46',\n",
    "        'BOMBARDIER Q400 (DH4)': 'DH8D',\n",
    "        'AIRBUS A330-200 (332)': 'A332',\n",
    "        'AIRBUS A330-300 (333)': 'A333',\n",
    "        'AIRBUS A350-900 (359)': 'A359',\n",
    "        'AIRBUS A380-800 (388)': 'A388',\n",
    "        'BOEING 737-700 (737)': 'B737',\n",
    "        'BOEING 737-800 (738)': 'B738',\n",
    "        'BOEING 737-900 (739)': 'B739',\n",
    "        'BOEING 747-400 (744)': 'B744',\n",
    "        'BOEING 777-200 (772)': 'B772',\n",
    "        'BOEING 777-300 (773)': 'B773',\n",
    "        'BOEING 787-8 (788)': 'B788',\n",
    "        'BOEING 787-9 (789)': 'B789',\n",
    "        'ATR 72-600 (ATR)': 'AT76',\n",
    "        'ATR 72-500 (ATR)': 'AT75',\n",
    "        'EMBRAER 175 (E75)': 'E75L',\n",
    "        'EMBRAER 190 (E90)': 'E190',\n",
    "        'EMBRAER 195 (E95)': 'E195',\n",
    "        # Add more mappings as needed\n",
    "    }\n",
    "    \n",
    "    if plane_type in plane_mapping:\n",
    "        return plane_mapping[plane_type]\n",
    "    \n",
    "    # Try to extract ICAO code from parentheses (e.g., \"Airbus A319 (319)\" -> \"A319\")\n",
    "    match = re.search(r'\\(([^)]+)\\)', plane_type)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    # If no match, try to infer ICAO code based on common patterns\n",
    "    if 'ATR 72' in plane_type:\n",
    "        return 'AT72'\n",
    "    elif 'ATR 42' in plane_type:\n",
    "        return 'AT46'\n",
    "    elif 'BOEING 737' in plane_type:\n",
    "        return 'B737'  # Default to B737 if specific variant not matched\n",
    "    elif 'AIRBUS A320' in plane_type:\n",
    "        return 'A320'\n",
    "    elif 'AIRBUS A321' in plane_type:\n",
    "        return 'A321'\n",
    "    \n",
    "    # If no match, return the original (cleaned) value\n",
    "    return plane_type.replace(' ', '')\n",
    "\n",
    "# Apply the function to standardize equipment_cd_icao\n",
    "seatguru_df['equipment_cd_icao'] = seatguru_df['equipment_cd_icao'].apply(extract_icao_code)\n",
    "\n",
    "# Step 3: Map carrier codes to full names in main dataset\n",
    "carrier_mapping = {\n",
    "    'VT': 'Vistara',\n",
    "    'JBW': 'Jubba Airways',\n",
    "    'AH': 'Air Algerie',\n",
    "    '5O': 'ASL Airlines France',\n",
    "    'SF': 'Tassili Airlines',\n",
    "    'TK': 'Turkish Airlines',\n",
    "    # Add more mappings based on main dataset carriers\n",
    "}\n",
    "\n",
    "# Step 4: Create merge key in SeatGuru data\n",
    "seatguru_df['merge_key'] = seatguru_df['carrier'].astype(str) + '_' + \\\n",
    "                          seatguru_df['equipment_cd_icao'].astype(str)\n",
    "\n",
    "# Step 5: Process the main dataset in chunks and merge with SeatGuru data\n",
    "first_chunk = True\n",
    "for chunk in pd.read_csv(main_file, chunksize=chunk_size):\n",
    "    # Map carrier codes to full names\n",
    "    chunk['carrier_mapped'] = chunk['carrier'].map(carrier_mapping).fillna(chunk['carrier'])\n",
    "    \n",
    "    # Create the same merge key in the main dataset chunk\n",
    "    chunk['merge_key'] = chunk['carrier_mapped'].astype(str) + '_' + \\\n",
    "                        chunk['equipment_cd_icao'].astype(str)\n",
    "\n",
    "    # Merge with SeatGuru data (left join)\n",
    "    chunk = chunk.merge(seatguru_df[['merge_key', 'amenities']], \n",
    "                        on='merge_key', \n",
    "                        how='left')\n",
    "\n",
    "    # Drop the merge_key and carrier_mapped columns as they are no longer needed\n",
    "    chunk = chunk.drop(columns=['merge_key', 'carrier_mapped'])\n",
    "\n",
    "    # Save to output file (append mode)\n",
    "    if first_chunk:\n",
    "        chunk.to_csv(output_file, index=False, mode='w')\n",
    "        first_chunk = False\n",
    "    else:\n",
    "        chunk.to_csv(output_file, index=False, mode='a', header=False)\n",
    "\n",
    "print(f\"Amenities column added. Updated dataset saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cf959d21-ffa2-4aa6-b875-3cfcd9babd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data with amenities (first 5 rows):\n",
      "  carrier  fltno fltsuffx depapt arrapt  distance_in_km equipment_cd_icao  \\\n",
      "0      VT    979      NaN    AAA    FAC        75.63898              AT72   \n",
      "1      VT    959      NaN    AAA    FAC        75.63898              AT72   \n",
      "2      VT    959      NaN    AAA    FAC        75.63898              AT72   \n",
      "3      VT    959      NaN    AAA    FAC        75.63898              AT72   \n",
      "4      VT    979      NaN    AAA    FAC        75.63898              AT72   \n",
      "\n",
      "   seatsfst  seatsbus  seatspremeco  seatseco  stops  co2e_per_passenger_kg  \\\n",
      "0         0         0             0        58      0                  11.44   \n",
      "1         0         4             0        52      0                  12.25   \n",
      "2         0         0             0        56      0                  11.44   \n",
      "3         0         4             0        52      0                  12.25   \n",
      "4         0         4             0        60      0                  12.15   \n",
      "\n",
      "  amenities  \n",
      "0       NaN  \n",
      "1       NaN  \n",
      "2       NaN  \n",
      "3       NaN  \n",
      "4       NaN  \n",
      "\n",
      "Data Quality Summary:\n",
      "Total rows: 7960863\n",
      "Rows with amenities data: 13184\n",
      "Rows without amenities data: 7947679\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = r\"D:\\work\\WDF_032025_CO2e_with_amenities.csv\"\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 10000\n",
    "\n",
    "# Initialize variables\n",
    "sample_data = None\n",
    "total_rows = 0\n",
    "rows_with_amenities = 0\n",
    "\n",
    "# Process the dataset in chunks\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "    # Get sample data (first 5 rows) if not already collected\n",
    "    if sample_data is None:\n",
    "        sample_data = chunk.head(5)\n",
    "    \n",
    "    # Count total rows\n",
    "    total_rows += len(chunk)\n",
    "    \n",
    "    # Count rows with non-NaN amenities\n",
    "    rows_with_amenities += chunk['amenities'].notna().sum()\n",
    "\n",
    "# Display sample data\n",
    "print(\"Sample data with amenities (first 5 rows):\")\n",
    "print(sample_data)\n",
    "\n",
    "# Display data quality summary\n",
    "print(\"\\nData Quality Summary:\")\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Rows with amenities data: {rows_with_amenities}\")\n",
    "print(f\"Rows without amenities data: {total_rows - rows_with_amenities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7d176ea0-6eeb-4258-bd40-416f245a730b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique equipment_cd_icao in SeatGuru dataset after standardization:\n",
      "['100', '300', '313', '318', '319', '320', '321', '32A', '32B', '32H', '32K', '32M', '32S', '332', '333', '339', '33D', '33E', '33H', '33K', '33P', '33R', '33S', '33T', '33V', '343', '345', '346', '350', '351', '359', '35A', '35G', '380', '388', '3N1', '717', '733', '734', '735', '736', '737', '738', '739', '73G', '73H', '73J', '73W', '73Z', '744', '747', '748', '74H', '752', '753', '757', '75D', '75G', '75P', '75S', '75Y', '762', '763', '764', '767', '76A', '76C', '76D', '76H/76Z', '76L', '76T/76W', '76W', '76Z', '772', '773', '777', '77A', '77A/77W', '77B', '77G', '77H', '77K', '77L', '77M', '77N/77W', '77W', '77X', '781', '788', '789', '7CB', '7HD', '7M8', '7M9', 'A319', 'A320', 'A321', 'A332', 'A333', 'A359', 'A388', 'AEROSPATIALEATR-42', 'AIRBUSA220-300', 'AIRBUSA330-200', 'AIRBUSA350-900ULR', 'AR8', 'AT46', 'AT5', 'AT7', 'AT72', 'AT75', 'AT76', 'ATR', 'ATR-72', 'B737', 'B738', 'B739', 'B744', 'B772', 'B773', 'B788', 'B789', 'BEECHCRAFT1900D', 'BOEING787-9', 'BOMBARDIERCRJ-100/200', 'BOMBARDIERCRJ-1000', 'BOMBARDIERCRJ-200', 'BOMBARDIERCRJ-700', 'BOMBARDIERCRJ-705', 'BOMBARDIERCRJ-900', 'BOMBARDIERQ100', 'BOMBARDIERQ300', 'BOMBARDIERQ400', 'CR2', 'CR7', 'CR9', 'CRJ', 'CRK', 'CS1', 'CS3', 'DH2', 'DH3', 'DH4', 'DH8D', 'DHT', 'E190', 'E195', 'E70', 'E75', 'E75L', 'E90', 'E95', 'EM2', 'EMBRAERE-170', 'EMBRAERE-170EUROPEAN', 'EMBRAERE-170UKDOMESTIC', 'EMBRAERE-175', 'EMBRAERE-190', 'EMBRAERE-190EUROPEAN', 'EMBRAERE-190UKDOMESTIC', 'EMBRAERE-195', 'EMBRAERERJ-145', 'EMBRAERERJ-190', 'ER3', 'ER4', 'ERD', 'ERJ', 'FOKKERF-100', 'HAWKERSIDDELEYHS748LAYOUT1', 'HAWKERSIDDELEYHS748LAYOUT2', 'HAWKERSIDDELEYHS748LAYOUT3', 'HAWKERSIDDELEYHS748LAYOUT4', 'M88', 'M90', 'SAAB340B', 'SF3', 'SFB', 'SU9', 'TWINOTTERDHC6']\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique equipment_cd_icao in SeatGuru dataset after standardization:\")\n",
    "print(sorted(seatguru_df['equipment_cd_icao'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d17eee68-e80f-4938-bebb-8d52eb6b6044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vistara entries in SeatGuru dataset:\n",
      "      carrier equipment_cd_icao amenities\n",
      "1239  Vistara               320      Food\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "seatguru_file = r\"C:\\Users\\amit.murmu\\Downloads\\airlines_with_amenities.xlsx\"\n",
    "\n",
    "# Load SeatGuru data\n",
    "seatguru_df = pd.read_excel(seatguru_file)\n",
    "\n",
    "# Rename columns\n",
    "seatguru_df = seatguru_df.rename(columns={\n",
    "    'Airline Name': 'carrier',\n",
    "    'Plane Types': 'equipment_cd_icao',\n",
    "    'Amenities': 'amenities'\n",
    "})\n",
    "\n",
    "# Standardize equipment_cd_icao (using the same function as before)\n",
    "def extract_icao_code(plane_type):\n",
    "    plane_type = str(plane_type).strip().upper()\n",
    "    plane_mapping = {\n",
    "        'AIRBUS A319 (319)': 'A319',\n",
    "        'AIRBUS A320 (320)': 'A320',\n",
    "        'AIRBUS A321 (321)': 'A321',\n",
    "        'ATR 42-600 (ATR)': 'AT46',\n",
    "        'BOMBARDIER Q400 (DH4)': 'DH8D',\n",
    "        'AIRBUS A330-200 (332)': 'A332',\n",
    "        'AIRBUS A330-300 (333)': 'A333',\n",
    "        'AIRBUS A350-900 (359)': 'A359',\n",
    "        'AIRBUS A380-800 (388)': 'A388',\n",
    "        'BOEING 737-700 (737)': 'B737',\n",
    "        'BOEING 737-800 (738)': 'B738',\n",
    "        'BOEING 737-900 (739)': 'B739',\n",
    "        'BOEING 747-400 (744)': 'B744',\n",
    "        'BOEING 777-200 (772)': 'B772',\n",
    "        'BOEING 777-300 (773)': 'B773',\n",
    "        'BOEING 787-8 (788)': 'B788',\n",
    "        'BOEING 787-9 (789)': 'B789',\n",
    "        'ATR 72-600 (ATR)': 'AT76',\n",
    "        'ATR 72-500 (ATR)': 'AT75',\n",
    "        'EMBRAER 175 (E75)': 'E75L',\n",
    "        'EMBRAER 190 (E90)': 'E190',\n",
    "        'EMBRAER 195 (E95)': 'E195',\n",
    "    }\n",
    "    if plane_type in plane_mapping:\n",
    "        return plane_mapping[plane_type]\n",
    "    match = re.search(r'\\(([^)]+)\\)', plane_type)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    if 'ATR 72' in plane_type:\n",
    "        return 'AT72'\n",
    "    elif 'ATR 42' in plane_type:\n",
    "        return 'AT46'\n",
    "    elif 'BOEING 737' in plane_type:\n",
    "        return 'B737'\n",
    "    elif 'AIRBUS A320' in plane_type:\n",
    "        return 'A320'\n",
    "    elif 'AIRBUS A321' in plane_type:\n",
    "        return 'A321'\n",
    "    return plane_type.replace(' ', '')\n",
    "\n",
    "seatguru_df['equipment_cd_icao'] = seatguru_df['equipment_cd_icao'].apply(extract_icao_code)\n",
    "\n",
    "# Filter for Vistara entries\n",
    "vistara_entries = seatguru_df[seatguru_df['carrier'] == 'Vistara']\n",
    "\n",
    "# Print Vistara entries\n",
    "print(\"Vistara entries in SeatGuru dataset:\")\n",
    "print(vistara_entries[['carrier', 'equipment_cd_icao', 'amenities']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "18c5ee0a-02d0-493e-be26-2d7f9d94830b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amenities column added. Updated dataset saved to D:\\work\\WDF_032025_CO2e_with_amenities.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "main_file = r\"D:\\work\\WDF_032025_CO2e.csv\"\n",
    "seatguru_file = r\"C:\\Users\\amit.murmu\\Downloads\\airlines_with_amenities.xlsx\"\n",
    "output_file = r\"D:\\work\\WDF_032025_CO2e_with_amenities.csv\"\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 10000\n",
    "\n",
    "# Step 1: Load SeatGuru data\n",
    "seatguru_df = pd.read_excel(seatguru_file)\n",
    "\n",
    "# Rename columns in SeatGuru data\n",
    "seatguru_df = seatguru_df.rename(columns={\n",
    "    'Airline Name': 'carrier',\n",
    "    'Plane Types': 'equipment_cd_icao',\n",
    "    'Amenities': 'amenities'\n",
    "})\n",
    "\n",
    "# Step 2: Standardize equipment_cd_icao in SeatGuru dataset\n",
    "def extract_icao_code(plane_type):\n",
    "    plane_type = str(plane_type).strip().upper()\n",
    "    plane_mapping = {\n",
    "        'AIRBUS A319 (319)': 'A319',\n",
    "        'AIRBUS A320 (320)': 'A320',\n",
    "        'AIRBUS A321 (321)': 'A321',\n",
    "        'ATR 42-600 (ATR)': 'AT46',\n",
    "        'BOMBARDIER Q400 (DH4)': 'DH8D',\n",
    "        'AIRBUS A330-200 (332)': 'A332',\n",
    "        'AIRBUS A330-300 (333)': 'A333',\n",
    "        'AIRBUS A350-900 (359)': 'A359',\n",
    "        'AIRBUS A380-800 (388)': 'A388',\n",
    "        'BOEING 737-700 (737)': 'B737',\n",
    "        'BOEING 737-800 (738)': 'B738',\n",
    "        'BOEING 737-900 (739)': 'B739',\n",
    "        'BOEING 747-400 (744)': 'B744',\n",
    "        'BOEING 777-200 (772)': 'B772',\n",
    "        'BOEING 777-300 (773)': 'B773',\n",
    "        'BOEING 787-8 (788)': 'B788',\n",
    "        'BOEING 787-9 (789)': 'B789',\n",
    "        'ATR 72-600 (ATR)': 'AT76',\n",
    "        'ATR 72-500 (ATR)': 'AT75',\n",
    "        'EMBRAER 175 (E75)': 'E75L',\n",
    "        'EMBRAER 190 (E90)': 'E190',\n",
    "        'EMBRAER 195 (E95)': 'E195',\n",
    "    }\n",
    "    if plane_type in plane_mapping:\n",
    "        return plane_mapping[plane_type]\n",
    "    match = re.search(r'\\(([^)]+)\\)', plane_type)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    if 'ATR 72' in plane_type:\n",
    "        return 'AT72'\n",
    "    elif 'ATR 42' in plane_type:\n",
    "        return 'AT46'\n",
    "    elif 'BOEING 737' in plane_type:\n",
    "        return 'B737'\n",
    "    elif 'AIRBUS A320' in plane_type:\n",
    "        return 'A320'\n",
    "    elif 'AIRBUS A321' in plane_type:\n",
    "        return 'A321'\n",
    "    return plane_type.replace(' ', '')\n",
    "\n",
    "seatguru_df['equipment_cd_icao'] = seatguru_df['equipment_cd_icao'].apply(extract_icao_code)\n",
    "\n",
    "# Step 3: Add manual entry for Vistara AT72\n",
    "# Assuming amenities for Vistara AT72 are \"Food, Wi-Fi\" (update based on actual data)\n",
    "new_entry = pd.DataFrame({\n",
    "    'carrier': ['Vistara'],\n",
    "    'equipment_cd_icao': ['AT72'],\n",
    "    'amenities': ['Food, Wi-Fi']\n",
    "})\n",
    "seatguru_df = pd.concat([seatguru_df, new_entry], ignore_index=True)\n",
    "\n",
    "# Step 4: Map carrier codes to full names in main dataset\n",
    "carrier_mapping = {\n",
    "    'VT': 'Vistara',\n",
    "    'JBW': 'Jubba Airways',\n",
    "    'AH': 'Air Algerie',\n",
    "    '5O': 'ASL Airlines France',\n",
    "    'SF': 'Tassili Airlines',\n",
    "    'TK': 'Turkish Airlines',\n",
    "    # Add more mappings based on main dataset carriers\n",
    "}\n",
    "\n",
    "# Step 5: Create merge key in SeatGuru data\n",
    "seatguru_df['merge_key'] = seatguru_df['carrier'].astype(str) + '_' + \\\n",
    "                          seatguru_df['equipment_cd_icao'].astype(str)\n",
    "\n",
    "# Step 6: Process the main dataset in chunks and merge with SeatGuru data\n",
    "first_chunk = True\n",
    "for chunk in pd.read_csv(main_file, chunksize=chunk_size):\n",
    "    # Map carrier codes to full names\n",
    "    chunk['carrier_mapped'] = chunk['carrier'].map(carrier_mapping).fillna(chunk['carrier'])\n",
    "    \n",
    "    # Create the same merge key in the main dataset chunk\n",
    "    chunk['merge_key'] = chunk['carrier_mapped'].astype(str) + '_' + \\\n",
    "                        chunk['equipment_cd_icao'].astype(str)\n",
    "\n",
    "    # Merge with SeatGuru data (left join)\n",
    "    chunk = chunk.merge(seatguru_df[['merge_key', 'amenities']], \n",
    "                        on='merge_key', \n",
    "                        how='left')\n",
    "\n",
    "    # Add fallback for unmatched rows\n",
    "    chunk['amenities'] = chunk['amenities'].fillna(\"No Amenities\")\n",
    "\n",
    "    # Drop the merge_key and carrier_mapped columns as they are no longer needed\n",
    "    chunk = chunk.drop(columns=['merge_key', 'carrier_mapped'])\n",
    "\n",
    "    # Save to output file (append mode)\n",
    "    if first_chunk:\n",
    "        chunk.to_csv(output_file, index=False, mode='w')\n",
    "        first_chunk = False\n",
    "    else:\n",
    "        chunk.to_csv(output_file, index=False, mode='a', header=False)\n",
    "\n",
    "print(f\"Amenities column added. Updated dataset saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "df2c6015-7522-4921-8ccc-da4aa43463d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carriers with matching amenities (and their counts):\n",
      "TK: 13184 rows\n",
      "VT: 5636 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = r\"D:\\work\\WDF_032025_CO2e_with_amenities.csv\"\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 10000\n",
    "\n",
    "# Initialize a dictionary to count matches per carrier\n",
    "carrier_matches = {}\n",
    "\n",
    "# Process the dataset in chunks\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "    # Count rows with non-default amenities per carrier\n",
    "    matches = chunk[chunk['amenities'].ne(\"No Amenities\")].groupby('carrier').size()\n",
    "    for carrier, count in matches.items():\n",
    "        carrier_matches[carrier] = carrier_matches.get(carrier, 0) + count\n",
    "\n",
    "# Display matching carriers\n",
    "print(\"Carriers with matching amenities (and their counts):\")\n",
    "for carrier, count in carrier_matches.items():\n",
    "    print(f\"{carrier}: {count} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6e888d07-ed3d-4aff-9f1a-a05e0a668abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Air Algerie entries in SeatGuru dataset:\n",
      "Empty DataFrame\n",
      "Columns: [carrier, equipment_cd_icao, amenities]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "seatguru_file = r\"C:\\Users\\amit.murmu\\Downloads\\airlines_with_amenities.xlsx\"\n",
    "\n",
    "# Load SeatGuru data\n",
    "seatguru_df = pd.read_excel(seatguru_file)\n",
    "\n",
    "# Rename columns\n",
    "seatguru_df = seatguru_df.rename(columns={\n",
    "    'Airline Name': 'carrier',\n",
    "    'Plane Types': 'equipment_cd_icao',\n",
    "    'Amenities': 'amenities'\n",
    "})\n",
    "\n",
    "# Standardize equipment_cd_icao (using the same function as before)\n",
    "def extract_icao_code(plane_type):\n",
    "    plane_type = str(plane_type).strip().upper()\n",
    "    plane_mapping = {\n",
    "        'AIRBUS A319 (319)': 'A319',\n",
    "        'AIRBUS A320 (320)': 'A320',\n",
    "        'AIRBUS A321 (321)': 'A321',\n",
    "        'ATR 42-600 (ATR)': 'AT46',\n",
    "        'BOMBARDIER Q400 (DH4)': 'DH8D',\n",
    "        'AIRBUS A330-200 (332)': 'A332',\n",
    "        'AIRBUS A330-300 (333)': 'A333',\n",
    "        'AIRBUS A350-900 (359)': 'A359',\n",
    "        'AIRBUS A380-800 (388)': 'A388',\n",
    "        'BOEING 737-700 (737)': 'B737',\n",
    "        'BOEING 737-800 (738)': 'B738',\n",
    "        'BOEING 737-900 (739)': 'B739',\n",
    "        'BOEING 747-400 (744)': 'B744',\n",
    "        'BOEING 777-200 (772)': 'B772',\n",
    "        'BOEING 777-300 (773)': 'B773',\n",
    "        'BOEING 787-8 (788)': 'B788',\n",
    "        'BOEING 787-9 (789)': 'B789',\n",
    "        'ATR 72-600 (ATR)': 'AT76',\n",
    "        'ATR 72-500 (ATR)': 'AT75',\n",
    "        'EMBRAER 175 (E75)': 'E75L',\n",
    "        'EMBRAER 190 (E90)': 'E190',\n",
    "        'EMBRAER 195 (E95)': 'E195',\n",
    "    }\n",
    "    if plane_type in plane_mapping:\n",
    "        return plane_mapping[plane_type]\n",
    "    match = re.search(r'\\(([^)]+)\\)', plane_type)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    if 'ATR 72' in plane_type:\n",
    "        return 'AT72'\n",
    "    elif 'ATR 42' in plane_type:\n",
    "        return 'AT46'\n",
    "    elif 'BOEING 737' in plane_type:\n",
    "        return 'B737'\n",
    "    elif 'AIRBUS A320' in plane_type:\n",
    "        return 'A320'\n",
    "    elif 'AIRBUS A321' in plane_type:\n",
    "        return 'A321'\n",
    "    return plane_type.replace(' ', '')\n",
    "\n",
    "seatguru_df['equipment_cd_icao'] = seatguru_df['equipment_cd_icao'].apply(extract_icao_code)\n",
    "\n",
    "# Filter for Air Algerie entries\n",
    "air_algerie_entries = seatguru_df[seatguru_df['carrier'] == 'Air Algerie']\n",
    "\n",
    "# Print Air Algerie entries\n",
    "print(\"Air Algerie entries in SeatGuru dataset:\")\n",
    "print(air_algerie_entries[['carrier', 'equipment_cd_icao', 'amenities']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b3faa1ff-a702-4b8f-93c7-10518e6b2e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amenities column added. Updated dataset saved to D:\\work\\WDF_032025_CO2e_with_amenities.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "main_file = r\"D:\\work\\WDF_032025_CO2e.csv\"\n",
    "seatguru_file = r\"C:\\Users\\amit.murmu\\Downloads\\airlines_with_amenities.xlsx\"\n",
    "output_file = r\"D:\\work\\WDF_032025_CO2e_with_amenities.csv\"\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 10000\n",
    "\n",
    "# Step 1: Load SeatGuru data\n",
    "seatguru_df = pd.read_excel(seatguru_file)\n",
    "\n",
    "# Rename columns in SeatGuru data\n",
    "seatguru_df = seatguru_df.rename(columns={\n",
    "    'Airline Name': 'carrier',\n",
    "    'Plane Types': 'equipment_cd_icao',\n",
    "    'Amenities': 'amenities'\n",
    "})\n",
    "\n",
    "# Step 2: Standardize equipment_cd_icao in SeatGuru dataset\n",
    "def extract_icao_code(plane_type):\n",
    "    plane_type = str(plane_type).strip().upper()\n",
    "    plane_mapping = {\n",
    "        'AIRBUS A319 (319)': 'A319',\n",
    "        'AIRBUS A320 (320)': 'A320',\n",
    "        'AIRBUS A321 (321)': 'A321',\n",
    "        'ATR 42-600 (ATR)': 'AT46',\n",
    "        'BOMBARDIER Q400 (DH4)': 'DH8D',\n",
    "        'AIRBUS A330-200 (332)': 'A332',\n",
    "        'AIRBUS A330-300 (333)': 'A333',\n",
    "        'AIRBUS A350-900 (359)': 'A359',\n",
    "        'AIRBUS A380-800 (388)': 'A388',\n",
    "        'BOEING 737-700 (737)': 'B737',\n",
    "        'BOEING 737-800 (738)': 'B738',\n",
    "        'BOEING 737-900 (739)': 'B739',\n",
    "        'BOEING 747-400 (744)': 'B744',\n",
    "        'BOEING 777-200 (772)': 'B772',\n",
    "        'BOEING 777-300 (773)': 'B773',\n",
    "        'BOEING 787-8 (788)': 'B788',\n",
    "        'BOEING 787-9 (789)': 'B789',\n",
    "        'ATR 72-600 (ATR)': 'AT76',\n",
    "        'ATR 72-500 (ATR)': 'AT75',\n",
    "        'EMBRAER 175 (E75)': 'E75L',\n",
    "        'EMBRAER 190 (E90)': 'E190',\n",
    "        'EMBRAER 195 (E95)': 'E195',\n",
    "    }\n",
    "    if plane_type in plane_mapping:\n",
    "        return plane_mapping[plane_type]\n",
    "    match = re.search(r'\\(([^)]+)\\)', plane_type)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    if 'ATR 72' in plane_type:\n",
    "        return 'AT72'\n",
    "    elif 'ATR 42' in plane_type:\n",
    "        return 'AT46'\n",
    "    elif 'BOEING 737' in plane_type:\n",
    "        return 'B737'\n",
    "    elif 'AIRBUS A320' in plane_type:\n",
    "        return 'A320'\n",
    "    elif 'AIRBUS A321' in plane_type:\n",
    "        return 'A321'\n",
    "    return plane_type.replace(' ', '')\n",
    "\n",
    "seatguru_df['equipment_cd_icao'] = seatguru_df['equipment_cd_icao'].apply(extract_icao_code)\n",
    "\n",
    "# Step 3: Add manual entries for missing carriers\n",
    "# Manual entries for Vistara (already added), Air Algerie, JBW, 5O, SF\n",
    "manual_entries = pd.DataFrame({\n",
    "    'carrier': [\n",
    "        'Vistara',           # Already added, keeping for completeness\n",
    "        'Air Algerie',       # For AH_B736\n",
    "        'Air Algerie',       # For AH_B738\n",
    "        'Air Algerie',       # For AH_AT72\n",
    "        'Jubba Airways',     # For JBW_F50\n",
    "        'ASL Airlines France', # For 5O_B737\n",
    "        'Tassili Airlines'   # For SF_DH8D\n",
    "    ],\n",
    "    'equipment_cd_icao': [\n",
    "        'AT72',              # Vistara AT72\n",
    "        'B736',              # Air Algerie B736\n",
    "        'B738',              # Air Algerie B738\n",
    "        'AT72',              # Air Algerie AT72\n",
    "        'F50',               # Jubba Airways F50\n",
    "        'B737',              # ASL Airlines France B737\n",
    "        'DH8D'               # Tassili Airlines DH8D\n",
    "    ],\n",
    "    'amenities': [\n",
    "        'Food, Wi-Fi',       # Vistara AT72 (assumed)\n",
    "        'Food',              # Air Algerie B736 (assumed)\n",
    "        'Food',              # Air Algerie B738 (assumed)\n",
    "        'Food',              # Air Algerie AT72 (assumed)\n",
    "        'No Amenities',      # Jubba Airways F50 (assumed)\n",
    "        'No Amenities',      # ASL Airlines France B737 (assumed)\n",
    "        'No Amenities'       # Tassili Airlines DH8D (assumed)\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Append manual entries to SeatGuru dataset\n",
    "seatguru_df = pd.concat([seatguru_df, manual_entries], ignore_index=True)\n",
    "\n",
    "# Step 4: Map carrier codes to full names in main dataset\n",
    "carrier_mapping = {\n",
    "    'VT': 'Vistara',\n",
    "    'JBW': 'Jubba Airways',\n",
    "    'AH': 'Air Algerie',\n",
    "    '5O': 'ASL Airlines France',\n",
    "    'SF': 'Tassili Airlines',\n",
    "    'TK': 'Turkish Airlines',\n",
    "    # Add more mappings based on main dataset carriers\n",
    "}\n",
    "\n",
    "# Step 5: Create merge key in SeatGuru data\n",
    "seatguru_df['merge_key'] = seatguru_df['carrier'].astype(str) + '_' + \\\n",
    "                          seatguru_df['equipment_cd_icao'].astype(str)\n",
    "\n",
    "# Step 6: Process the main dataset in chunks and merge with SeatGuru data\n",
    "first_chunk = True\n",
    "for chunk in pd.read_csv(main_file, chunksize=chunk_size):\n",
    "    # Map carrier codes to full names\n",
    "    chunk['carrier_mapped'] = chunk['carrier'].map(carrier_mapping).fillna(chunk['carrier'])\n",
    "    \n",
    "    # Create the same merge key in the main dataset chunk\n",
    "    chunk['merge_key'] = chunk['carrier_mapped'].astype(str) + '_' + \\\n",
    "                        chunk['equipment_cd_icao'].astype(str)\n",
    "\n",
    "    # Merge with SeatGuru data (left join)\n",
    "    chunk = chunk.merge(seatguru_df[['merge_key', 'amenities']], \n",
    "                        on='merge_key', \n",
    "                        how='left')\n",
    "\n",
    "    # Add fallback for unmatched rows\n",
    "    chunk['amenities'] = chunk['amenities'].fillna(\"No Amenities\")\n",
    "\n",
    "    # Drop the merge_key and carrier_mapped columns as they are no longer needed\n",
    "    chunk = chunk.drop(columns=['merge_key', 'carrier_mapped'])\n",
    "\n",
    "    # Save to output file (append mode)\n",
    "    if first_chunk:\n",
    "        chunk.to_csv(output_file, index=False, mode='w')\n",
    "        first_chunk = False\n",
    "    else:\n",
    "        chunk.to_csv(output_file, index=False, mode='a', header=False)\n",
    "\n",
    "print(f\"Amenities column added. Updated dataset saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "798031b7-5620-4ecd-909c-b3237ed7c8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amit.murmu\\AppData\\Local\\Temp\\ipykernel_11624\\1695379915.py:93: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp_df = pd.read_csv(temp_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OTP% column added. Updated dataset saved to D:\\work\\WDF_032025_CO2e_with_amenities_otp.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# File paths\n",
    "input_file = r\"D:\\work\\WDF_032025_CO2e_with_amenities.csv\"\n",
    "output_file = r\"D:\\work\\WDF_032025_CO2e_with_amenities_otp.csv\"\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 10000\n",
    "\n",
    "# Step 1: Helper functions\n",
    "def generate_random_date():\n",
    "    # Generate a random date in March 2025\n",
    "    start_date = datetime(2025, 3, 1)\n",
    "    end_date = datetime(2025, 3, 31)\n",
    "    delta = end_date - start_date\n",
    "    random_days = np.random.randint(0, delta.days + 1)\n",
    "    return start_date + timedelta(days=random_days)\n",
    "\n",
    "def generate_random_time():\n",
    "    # Generate a random time (HH:MM)\n",
    "    hours = np.random.randint(0, 24)\n",
    "    minutes = np.random.randint(0, 60)\n",
    "    return f\"{hours:02d}:{minutes:02d}\"\n",
    "\n",
    "def calculate_flight_duration(distance_km):\n",
    "    # Estimate flight duration based on distance\n",
    "    # Average speed: 800 km/h\n",
    "    # Add 30 minutes for taxi time\n",
    "    flight_hours = distance_km / 800\n",
    "    total_minutes = int(flight_hours * 60) + 30  # Convert to minutes and add taxi time\n",
    "    return total_minutes\n",
    "\n",
    "def generate_delayed_time(scheduled_time, delay_prob):\n",
    "    # Parse scheduled time (HH:MM)\n",
    "    scheduled = datetime.strptime(scheduled_time, \"%H:%M\")\n",
    "    \n",
    "    # Decide delay based on probability distribution\n",
    "    rand = np.random.random()\n",
    "    if rand < 0.7:  # 70% chance of no delay\n",
    "        delay_minutes = 0\n",
    "    elif rand < 0.9:  # 20% chance of minor delay (15-30 minutes)\n",
    "        delay_minutes = np.random.randint(15, 31)\n",
    "    else:  # 10% chance of major delay (30-60 minutes)\n",
    "        delay_minutes = np.random.randint(30, 61)\n",
    "    \n",
    "    delayed = scheduled + timedelta(minutes=delay_minutes)\n",
    "    return delayed.strftime(\"%H:%M\")\n",
    "\n",
    "def is_on_time(scheduled, estimated):\n",
    "    # Check if estimated time is within 15 minutes of scheduled time\n",
    "    scheduled_dt = datetime.strptime(scheduled, \"%H:%M\")\n",
    "    estimated_dt = datetime.strptime(estimated, \"%H:%M\")\n",
    "    delta = abs((estimated_dt - scheduled_dt).total_seconds()) / 60  # Difference in minutes\n",
    "    return delta <= 15\n",
    "\n",
    "# Step 2: Process the dataset in chunks and add temporary columns\n",
    "temp_file = r\"D:\\work\\temp_with_timings.csv\"\n",
    "first_chunk = True\n",
    "for chunk in pd.read_csv(input_file, chunksize=chunk_size):\n",
    "    # Add flight date\n",
    "    chunk['flight_date'] = [generate_random_date().strftime(\"%Y-%m-%d\") for _ in range(len(chunk))]\n",
    "    \n",
    "    # Add STD (Scheduled Time of Departure)\n",
    "    chunk['STD'] = [generate_random_time() for _ in range(len(chunk))]\n",
    "    \n",
    "    # Calculate STA (Scheduled Time of Arrival)\n",
    "    chunk['duration_minutes'] = chunk['distance_in_km'].apply(calculate_flight_duration)\n",
    "    chunk['STA'] = chunk.apply(lambda row: (datetime.strptime(row['STD'], \"%H:%M\") + \n",
    "                                            timedelta(minutes=row['duration_minutes'])).strftime(\"%H:%M\"), axis=1)\n",
    "    \n",
    "    # Generate ETD (Estimated Time of Departure)\n",
    "    chunk['ETD'] = chunk['STD'].apply(lambda x: generate_delayed_time(x, delay_prob=0.7))\n",
    "    \n",
    "    # Generate ETA (Estimated Time of Arrival)\n",
    "    chunk['ETA'] = chunk['STA'].apply(lambda x: generate_delayed_time(x, delay_prob=0.7))\n",
    "    \n",
    "    # Check if flight is on-time\n",
    "    chunk['on_time_departure'] = chunk.apply(lambda row: is_on_time(row['STD'], row['ETD']), axis=1)\n",
    "    chunk['on_time_arrival'] = chunk.apply(lambda row: is_on_time(row['STA'], row['ETA']), axis=1)\n",
    "    chunk['on_time'] = chunk['on_time_departure'] & chunk['on_time_arrival']\n",
    "    \n",
    "    # Save to temporary file (append mode)\n",
    "    if first_chunk:\n",
    "        chunk.to_csv(temp_file, index=False, mode='w')\n",
    "        first_chunk = False\n",
    "    else:\n",
    "        chunk.to_csv(temp_file, index=False, mode='a', header=False)\n",
    "\n",
    "# Step 3: Calculate daily OTP% per flight\n",
    "# Load the temporary file\n",
    "temp_df = pd.read_csv(temp_file)\n",
    "\n",
    "# Create a flight identifier (carrier + fltno)\n",
    "temp_df['flight_id'] = temp_df['carrier'].astype(str) + '_' + temp_df['fltno'].astype(str)\n",
    "\n",
    "# Group by flight_id and flight_date to calculate daily OTP%\n",
    "daily_otp = temp_df.groupby(['flight_id', 'flight_date'])['on_time'].mean().reset_index()\n",
    "daily_otp['otp%'] = (daily_otp['on_time'] * 100).round(2)\n",
    "\n",
    "# Step 4: Merge OTP% back into the main dataset\n",
    "first_chunk = True\n",
    "for chunk in pd.read_csv(temp_file, chunksize=chunk_size):\n",
    "    # Create flight_id in the chunk\n",
    "    chunk['flight_id'] = chunk['carrier'].astype(str) + '_' + chunk['fltno'].astype(str)\n",
    "    \n",
    "    # Merge with daily_otp to get otp%\n",
    "    chunk = chunk.merge(daily_otp[['flight_id', 'flight_date', 'otp%']], \n",
    "                        on=['flight_id', 'flight_date'], \n",
    "                        how='left')\n",
    "    \n",
    "    # Drop temporary columns\n",
    "    chunk = chunk.drop(columns=['flight_date', 'STD', 'STA', 'ETD', 'ETA', \n",
    "                                'duration_minutes', 'on_time_departure', \n",
    "                                'on_time_arrival', 'on_time', 'flight_id'])\n",
    "    \n",
    "    # Save to final output file (append mode)\n",
    "    if first_chunk:\n",
    "        chunk.to_csv(output_file, index=False, mode='w')\n",
    "        first_chunk = False\n",
    "    else:\n",
    "        chunk.to_csv(output_file, index=False, mode='a', header=False)\n",
    "\n",
    "print(f\"OTP% column added. Updated dataset saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ec786d59-56d1-400a-9b4f-2bd76513264b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data with OTP% (first 5 rows):\n",
      "  carrier  fltno fltsuffx depapt arrapt  distance_in_km equipment_cd_icao  \\\n",
      "0      VT    979      NaN    AAA    FAC        75.63898              AT72   \n",
      "1      VT    959      NaN    AAA    FAC        75.63898              AT72   \n",
      "2      VT    959      NaN    AAA    FAC        75.63898              AT72   \n",
      "3      VT    959      NaN    AAA    FAC        75.63898              AT72   \n",
      "4      VT    979      NaN    AAA    FAC        75.63898              AT72   \n",
      "\n",
      "   seatsfst  seatsbus  seatspremeco  seatseco  stops  co2e_per_passenger_kg  \\\n",
      "0         0         0             0        58      0                  11.44   \n",
      "1         0         4             0        52      0                  12.25   \n",
      "2         0         0             0        56      0                  11.44   \n",
      "3         0         4             0        52      0                  12.25   \n",
      "4         0         4             0        60      0                  12.15   \n",
      "\n",
      "     amenities   otp%  \n",
      "0  Food, Wi-Fi   50.0  \n",
      "1  Food, Wi-Fi   40.0  \n",
      "2  Food, Wi-Fi   75.0  \n",
      "3  Food, Wi-Fi  100.0  \n",
      "4  Food, Wi-Fi   60.0  \n",
      "\n",
      "Data Quality Summary:\n",
      "Total rows: 7960863\n",
      "Average OTP%: 50.75%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = r\"D:\\work\\WDF_032025_CO2e_with_amenities_otp.csv\"\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 10000\n",
    "\n",
    "# Initialize variables\n",
    "sample_data = None\n",
    "total_rows = 0\n",
    "avg_otp = 0\n",
    "\n",
    "# Process the dataset in chunks\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "    # Get sample data (first 5 rows) if not already collected\n",
    "    if sample_data is None:\n",
    "        sample_data = chunk.head(5)\n",
    "    \n",
    "    # Count total rows\n",
    "    total_rows += len(chunk)\n",
    "    \n",
    "    # Calculate average OTP% (for quality check)\n",
    "    avg_otp += chunk['otp%'].sum()\n",
    "\n",
    "# Calculate overall average OTP%\n",
    "avg_otp = avg_otp / total_rows\n",
    "\n",
    "# Display sample data\n",
    "print(\"Sample data with OTP% (first 5 rows):\")\n",
    "print(sample_data)\n",
    "\n",
    "# Display data quality summary\n",
    "print(\"\\nData Quality Summary:\")\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Average OTP%: {avg_otp:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2a8c81b2-c4f4-4d8b-962f-3e4daf6b5e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Amenities Distribution:\n",
      "amenities\n",
      "No Amenities                        7936540\n",
      "Food, Tv, Headphones                   7673\n",
      "Food, Wi-Fi                            5636\n",
      "Food, Wifi, Elec, Tv, Headphones       5511\n",
      "Food                                   5503\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Updated Amenities Distribution:\n",
      "amenities\n",
      "Food, Wi-Fi    7800989\n",
      "Food            159874\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Updated dataset saved to: D:\\work\\WDF_032025_CO2e_with_amenities_otp_updated.csv\n",
      "\n",
      "Sample data with updated amenities (first 5 rows):\n",
      "  carrier  fltno fltsuffx depapt arrapt  distance_in_km equipment_cd_icao  \\\n",
      "0      VT    979      NaN    AAA    FAC        75.63898              AT72   \n",
      "1      VT    959      NaN    AAA    FAC        75.63898              AT72   \n",
      "2      VT    959      NaN    AAA    FAC        75.63898              AT72   \n",
      "3      VT    959      NaN    AAA    FAC        75.63898              AT72   \n",
      "4      VT    979      NaN    AAA    FAC        75.63898              AT72   \n",
      "\n",
      "   seatsfst  seatsbus  seatspremeco  seatseco  stops  co2e_per_passenger_kg  \\\n",
      "0         0         0             0        58      0                  11.44   \n",
      "1         0         4             0        52      0                  12.25   \n",
      "2         0         0             0        56      0                  11.44   \n",
      "3         0         4             0        52      0                  12.25   \n",
      "4         0         4             0        60      0                  12.15   \n",
      "\n",
      "  amenities   otp%  \n",
      "0      Food   50.0  \n",
      "1      Food   40.0  \n",
      "2      Food   75.0  \n",
      "3      Food  100.0  \n",
      "4      Food   60.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the dataset\n",
    "DATASET_PATH = r\"D:\\work\\WDF_032025_CO2e_with_amenities_otp.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(DATASET_PATH, dtype={'fltsuffx': str})\n",
    "\n",
    "# Step 1: Check current amenities distribution\n",
    "print(\"Current Amenities Distribution:\")\n",
    "print(df['amenities'].value_counts(dropna=False))\n",
    "\n",
    "# Step 2: Define amenities mapping based on carrier and aircraft type\n",
    "def assign_amenities(row):\n",
    "    carrier = row['carrier']\n",
    "    aircraft = row['equipment_cd_icao']\n",
    "    \n",
    "    # Low-cost airlines (typically no Wi-Fi, only Food)\n",
    "    low_cost_airlines = ['6E', 'QP', 'SG']  # IndiGo, Akasa Air, SpiceJet\n",
    "    # Full-service airlines (Food, Wi-Fi, sometimes Entertainment)\n",
    "    full_service_airlines = ['AI', 'VT', 'UK']  # Air India, Vistara\n",
    "    \n",
    "    # Smaller aircraft (e.g., turboprops like AT72) typically don't have Wi-Fi\n",
    "    small_aircraft = ['AT72', 'ATR', 'DH8D']\n",
    "    \n",
    "    if carrier in low_cost_airlines:\n",
    "        return \"Food\"  # Low-cost airlines mein sirf Food (purchasable)\n",
    "    elif carrier in full_service_airlines:\n",
    "        if aircraft in small_aircraft:\n",
    "            return \"Food\"  # Smaller aircraft mein Wi-Fi nahi hota\n",
    "        else:\n",
    "            return \"Food, Wi-Fi\"  # Full-service airlines mein Food aur Wi-Fi\n",
    "    else:\n",
    "        # Default for other airlines (e.g., international or unknown carriers)\n",
    "        if aircraft in small_aircraft:\n",
    "            return \"Food\"\n",
    "        else:\n",
    "            return \"Food, Wi-Fi\"\n",
    "\n",
    "# Step 3: Apply the amenities mapping to the dataset\n",
    "df['amenities'] = df.apply(assign_amenities, axis=1)\n",
    "\n",
    "# Step 4: Verify the updated amenities distribution\n",
    "print(\"\\nUpdated Amenities Distribution:\")\n",
    "print(df['amenities'].value_counts(dropna=False))\n",
    "\n",
    "# Step 5: Save the updated dataset\n",
    "OUTPUT_PATH = r\"D:\\work\\WDF_032025_CO2e_with_amenities_otp_updated.csv\"\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"\\nUpdated dataset saved to: {OUTPUT_PATH}\")\n",
    "\n",
    "# Step 6: Show sample data (first 5 rows)\n",
    "print(\"\\nSample data with updated amenities (first 5 rows):\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9e94b379-48d5-4081-9fa7-2780d23660dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Carriers in Dataset:\n",
      "['0B', '0N', '2A', '2E', '2I', '2J', '2L', '2M', '2N', '2O', '2R', '2S', '2T', '2U', '2W', '2Z', '3A', '3C', '3F', '3H', '3K', '3L', '3M', '3O', '3P', '3R', '3T', '3U', '3Z', '4A', '4B', '4F', '4G', '4I', '4N', '4O', '4R', '4S', '4T', '4U', '4W', '4Y', '4Z', '5A', '5E', '5F', '5G', '5I', '5J', '5L', '5M', '5N', '5O', '5R', '5T', '5U', '5V', '5W', '5Z', '6A', '6D', '6E', '6F', '6G', '6H', '6I', '6J', '6R', '78V', '7BZ', '7C', '7F', '7G', '7H', '7O', '7P', '7R', '7S', '7V', '7Y', '7Z', '8B', '8D', '8E', '8F', '8G', '8J', '8L', '8M', '8N', '8P', '8S', '8T', '8U', '8V', '8W', '8Z', '9B', '9C', '9D', '9F', '9H', '9I', '9K', '9M', '9N', '9P', '9Q', '9R', '9S', '9V', '9X', 'A1', 'A2', 'A3', 'A4', 'A6', 'A8', 'A9', 'AA', 'AAT', 'AC', 'AD', 'AE', 'AF', 'AGU', 'AH', 'AI', 'AJ', 'AK', 'AKU', 'AM', 'AMT', 'AN', 'AQ', 'AR', 'AS', 'AT', 'ATM', 'AUD', 'AV', 'AW', 'AXK', 'AY', 'AZ', 'B0', 'B2', 'B3', 'B4', 'B6', 'B7', 'B9', 'BA', 'BB', 'BC', 'BF', 'BG', 'BI', 'BJ', 'BK', 'BL', 'BLX', 'BM', 'BP', 'BQ', 'BR', 'BS', 'BT', 'BU', 'BV', 'BW', 'BX', 'BZ', 'C2', 'C3', 'C6', 'C7', 'C8', 'CA', 'CAT', 'CD', 'CE', 'CG', 'CI', 'CM', 'CN', 'CON', 'CPN', 'CQ', 'CU', 'CVZ', 'CWM', 'CX', 'CY', 'CZ', 'D2', 'D3', 'D7', 'D8', 'D9', 'DCT', 'DD', 'DE', 'DG', 'DI', 'DJ', 'DK', 'DL', 'DM', 'DMS', 'DN', 'DO', 'DR', 'DT', 'DU', 'DV', 'DX', 'DY', 'DZ', 'DZD', 'E2', 'E3', 'E4', 'E5', 'E9', 'EGU', 'EI', 'EJ', 'EK', 'EKA', 'EN', 'ENT', 'EO', 'EP', 'EQ', 'ER', 'ES', 'ET', 'EU', 'EW', 'EY', 'F2', 'F3', 'F4', 'F8', 'F9', 'FA', 'FB', 'FC', 'FD', 'FG', 'FH', 'FI', 'FJ', 'FL', 'FLG', 'FM', 'FN', 'FNA', 'FO', 'FP', 'FQ', 'FR', 'FRC', 'FS', 'FTR', 'FU', 'FW', 'FY', 'FZ', 'G3', 'G4', 'G5', 'G7', 'G9', 'GA', 'GAL', 'GD', 'GE', 'GF', 'GJ', 'GK', 'GL', 'GM', 'GMT', 'GP', 'GQ', 'GR', 'GS', 'GT', 'GUM', 'GV', 'GW', 'GX', 'GY', 'GZ', 'H2', 'H4', 'H5', 'H7', 'H8', 'H9', 'HA', 'HB', 'HBR', 'HC', 'HD', 'HF', 'HH', 'HI', 'HK', 'HM', 'HN', 'HO', 'HP', 'HR', 'HT', 'HTY', 'HU', 'HV', 'HW', 'HX', 'HY', 'HZ', 'I4', 'I6', 'I7', 'I8', 'IA', 'IB', 'IC', 'ID', 'IE', 'IF', 'IFY', 'II', 'IJ', 'IK', 'IL', 'IN', 'IO', 'IOS', 'IP', 'IQ', 'IR', 'IRU', 'IRZ', 'IS', 'IT', 'IU', 'IV', 'IW', 'IX', 'IY', 'IZ', 'IZG', 'J2', 'J4', 'J5', 'J7', 'J8', 'J9', 'JA', 'JBA', 'JBW', 'JD', 'JH', 'JI', 'JL', 'JM', 'JN', 'JQ', 'JR', 'JS', 'JT', 'JTU', 'JU', 'JV', 'JWX', 'JX', 'JY', 'K2', 'K3', 'K6', 'K7', 'K9', 'KA', 'KAE', 'KAI', 'KB', 'KC', 'KE', 'KG', 'KGN', 'KGZ', 'KHF', 'KK', 'KL', 'KM', 'KMA', 'KMW', 'KN', 'KNW', 'KP', 'KQ', 'KR', 'KRU', 'KT', 'KU', 'KV', 'KX', 'KY', 'L4', 'L5', 'L6', 'L8', 'L9', 'LA', 'LC', 'LE', 'LF', 'LG', 'LH', 'LID', 'LJ', 'LK', 'LM', 'LN', 'LNH', 'LO', 'LS', 'LT', 'LTR', 'LX', 'LY', 'LZ', 'M0', 'M5', 'MD', 'ME', 'MF', 'MH', 'MK', 'MKB', 'ML', 'MM', 'MO', 'MR', 'MS', 'MU', 'MUS', 'MX', 'MZ', 'N0', 'N2', 'N3', 'N4', 'N5', 'N9', 'NAL', 'NB', 'NE', 'NEA', 'NF', 'NH', 'NK', 'NM', 'NO', 'NP', 'NQ', 'NR', 'NS', 'NT', 'NU', 'NX', 'NZ', 'OA', 'OAO', 'OB', 'OC', 'OD', 'OF', 'OG', 'OI', 'OJ', 'OL', 'OM', 'ON', 'OP', 'OQ', 'OR', 'OS', 'OU', 'OV', 'OW', 'OZ', 'P0', 'P2', 'P4', 'P5', 'P6', 'PA', 'PB', 'PC', 'PD', 'PE', 'PF', 'PG', 'PJ', 'PK', 'PL', 'PM', 'PN', 'PNP', 'PNX', 'PP', 'PR', 'PTK', 'PU', 'PV', 'PW', 'PX', 'PY', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q9', 'QC', 'QF', 'QG', 'QH', 'QI', 'QL', 'QN', 'QP', 'QQ', 'QR', 'QS', 'QU', 'QV', 'QW', 'QZ', 'R2', 'R3', 'R4', 'R5', 'RA', 'RB', 'RC', 'RF', 'RJ', 'RK', 'RKA', 'RLY', 'RN', 'RNG', 'RO', 'RQ', 'RRV', 'RS', 'RSI', 'RT', 'RTU', 'RVP', 'RW', 'RY', 'RZ', 'S0', 'S2', 'S4', 'S5', 'S6', 'S7', 'S8', 'SA', 'SAW', 'SB', 'SC', 'SEN', 'SF', 'SG', 'SH', 'SI', 'SJ', 'SJN', 'SK', 'SL', 'SM', 'SN', 'SOA', 'SP', 'SQ', 'SQS', 'SR', 'SS', 'SSU', 'ST', 'SU', 'SV', 'SVD', 'SX', 'SY', 'SZ', 'T3', 'T5', 'T6', 'T7', 'T9', 'TB', 'TBZ', 'TC', 'TG', 'TGY', 'THU', 'TI', 'TJ', 'TK', 'TL', 'TM', 'TN', 'TO', 'TOM', 'TP', 'TR', 'TS', 'TSG', 'TU', 'TV', 'TW', 'TWC', 'TX', 'TY', 'U2', 'U4', 'U5', 'U6', 'U8', 'UA', 'UB', 'UD', 'UG', 'UI', 'UJ', 'UL', 'UM', 'UN', 'UO', 'UP', 'UQ', 'UR', 'US', 'UT', 'UU', 'UX', 'UZ', 'V0', 'V4', 'V5', 'V7', 'V8', 'VA', 'VB', 'VC', 'VE', 'VF', 'VGV', 'VJ', 'VK', 'VL', 'VN', 'VO', 'VP', 'VQ', 'VR', 'VS', 'VT', 'VU', 'VY', 'VZ', 'W1', 'W2', 'W3', 'W4', 'W5', 'W6', 'W7', 'W9', 'WA', 'WB', 'WF', 'WG', 'WK', 'WM', 'WN', 'WP', 'WS', 'WT', 'WU', 'WV', 'WW', 'WY', 'WZ', 'X3', 'X4', 'XC', 'XE', 'XG', 'XJ', 'XK', 'XLL', 'XN', 'XP', 'XQ', 'XR', 'XY', 'XZ', 'Y2', 'Y4', 'Y7', 'Y8', 'YB', 'YC', 'YI', 'YK', 'YL', 'YN', 'YP', 'YR', 'YS', 'YT', 'YU', 'YW', 'Z0', 'Z2', 'ZA', 'ZB', 'ZD', 'ZE', 'ZF', 'ZG', 'ZH', 'ZL', 'ZN', 'ZP', 'ZV']\n",
      "\n",
      "Unique Aircraft Types in Dataset:\n",
      "['*', 'A109', 'A139', 'A148', 'A19N', 'A20N', 'A21N', 'A306', 'A30B', 'A318', 'A319', 'A320', 'A321', 'A332', 'A333', 'A338', 'A339', 'A342', 'A343', 'A346', 'A359', 'A35K', 'A388', 'AJ27', 'AN24', 'AN26', 'AN38', 'AS50', 'AT43', 'AT72', 'B06', 'B190', 'B37M', 'B38M', 'B39M', 'B462', 'B463', 'B712', 'B732', 'B733', 'B734', 'B735', 'B736', 'B737', 'B738', 'B739', 'B744', 'B748', 'B752', 'B753', 'B762', 'B763', 'B764', 'B772', 'B773', 'B77L', 'B77W', 'B788', 'B789', 'B78X', 'BCS1', 'BCS3', 'BN2P', 'C208', 'CL30', 'CRJ1', 'CRJ2', 'CRJ7', 'CRJ9', 'CRJX', 'D228', 'D328', 'DA42', 'DA62', 'DH3T', 'DH8A', 'DH8B', 'DH8C', 'DH8D', 'DHC2', 'DHC4', 'DHC6', 'DHC7', 'E110', 'E120', 'E135', 'E145', 'E170', 'E190', 'E195', 'E75L', 'EC30', 'EC55', 'F100', 'F50', 'F70', 'J328', 'JS31', 'JS32', 'JS41', 'L410', 'MD82', 'MD83', 'MI8', 'P212', 'PC12', 'RJ85', 'S76', 'SB20', 'SF34', 'SU95', 'SW4', 'T154', 'T204', 'TRIS', 'YK40', 'YK42']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the dataset\n",
    "DATASET_PATH = r\"D:\\work\\WDF_032025_CO2e_with_amenities_otp.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(DATASET_PATH, dtype={'fltsuffx': str})\n",
    "\n",
    "# Extract unique carriers and aircraft types\n",
    "unique_carriers = sorted(df['carrier'].unique())\n",
    "unique_aircraft = sorted(df['equipment_cd_icao'].unique())\n",
    "\n",
    "# Print the unique values\n",
    "print(\"Unique Carriers in Dataset:\")\n",
    "print(unique_carriers)\n",
    "print(\"\\nUnique Aircraft Types in Dataset:\")\n",
    "print(unique_aircraft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "78678576-958c-4f9a-9af1-97520353f46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Amenities Distribution:\n",
      "amenities\n",
      "No Amenities                        7936540\n",
      "Food, Tv, Headphones                   7673\n",
      "Food, Wi-Fi                            5636\n",
      "Food, Wifi, Elec, Tv, Headphones       5511\n",
      "Food                                   5503\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Updated Amenities Distribution:\n",
      "amenities\n",
      "Food, Entertainment    6096662\n",
      "Food                   1864201\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Updated dataset saved to: D:\\work\\WDF_032025_CO2e_with_amenities_otp_updated.csv\n",
      "\n",
      "Sample data with updated amenities (first 5 rows):\n",
      "  carrier  fltno fltsuffx depapt arrapt  distance_in_km equipment_cd_icao  \\\n",
      "0      VT    979      NaN    AAA    FAC        75.63898              AT72   \n",
      "1      VT    959      NaN    AAA    FAC        75.63898              AT72   \n",
      "2      VT    959      NaN    AAA    FAC        75.63898              AT72   \n",
      "3      VT    959      NaN    AAA    FAC        75.63898              AT72   \n",
      "4      VT    979      NaN    AAA    FAC        75.63898              AT72   \n",
      "\n",
      "   seatsfst  seatsbus  seatspremeco  seatseco  stops  co2e_per_passenger_kg  \\\n",
      "0         0         0             0        58      0                  11.44   \n",
      "1         0         4             0        52      0                  12.25   \n",
      "2         0         0             0        56      0                  11.44   \n",
      "3         0         4             0        52      0                  12.25   \n",
      "4         0         4             0        60      0                  12.15   \n",
      "\n",
      "  amenities   otp%  \n",
      "0      Food   50.0  \n",
      "1      Food   40.0  \n",
      "2      Food   75.0  \n",
      "3      Food  100.0  \n",
      "4      Food   60.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the dataset\n",
    "DATASET_PATH = r\"D:\\work\\WDF_032025_CO2e_with_amenities_otp.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(DATASET_PATH, dtype={'fltsuffx': str})\n",
    "\n",
    "# Step 1: Check current amenities distribution\n",
    "print(\"Current Amenities Distribution:\")\n",
    "print(df['amenities'].value_counts(dropna=False))\n",
    "\n",
    "# Step 2: Define comprehensive mapping\n",
    "# IATA-based airline type mapping\n",
    "low_cost_airlines = [\n",
    "    '6E', 'QP', 'SG', 'G8', 'I5',  # Indian low-cost\n",
    "    'AK', 'D7', 'FD', 'FR', 'U2', 'W6', 'NK', 'F9', 'B6', 'WN',  # International low-cost\n",
    "    'VY', 'DY', 'EW', 'JQ', '3K', 'OD', 'IX', 'G9', 'FZ'\n",
    "]\n",
    "\n",
    "full_service_airlines = [\n",
    "    'AI', 'VT', 'UK', '9I',  # Indian full-service\n",
    "    'EK', 'SQ', 'QR', 'LH', 'BA', 'AF', 'KL', 'DL', 'AA', 'UA',  # International full-service\n",
    "    'CX', 'JL', 'NH', 'KE', 'OZ', 'TG', 'MH', 'GA', 'VN', 'PR',\n",
    "    'QF', 'NZ', 'ET', 'TK', 'MS', 'SV', 'EY', 'KU', 'WY', 'UL',\n",
    "    'IB', 'AY', 'LO', 'OS', 'LX', 'SK', 'TP', 'AC', 'AV', 'LA',\n",
    "    'AM', 'AR', 'CA', 'CZ', 'MU', 'HU'\n",
    "]\n",
    "\n",
    "# ICAO-based aircraft type mapping\n",
    "small_aircraft = [\n",
    "    # Turboprops\n",
    "    'AT72', 'AT43', 'DH8A', 'DH8B', 'DH8C', 'DH8D', 'DHC2', 'DHC4', 'DHC6', 'DHC7',\n",
    "    'BN2P', 'SF34', 'JS31', 'JS32', 'JS41', 'L410', 'SW4',\n",
    "    # Regional Jets\n",
    "    'CRJ1', 'CRJ2', 'CRJ7', 'CRJ9', 'CRJX', 'E110', 'E120', 'E135', 'E145', 'E170',\n",
    "    'B190', 'D228', 'D328', 'J328', 'RJ85',\n",
    "    # Other Small Aircraft\n",
    "    'C208', 'PC12', 'TRIS', 'AN24', 'AN26', 'AN38', 'YK40', 'YK42',\n",
    "    # Helicopters\n",
    "    'A109', 'A139', 'B06', 'EC30', 'EC55', 'S76', 'MI8'\n",
    "]\n",
    "\n",
    "# Step 3: Define amenities assignment logic\n",
    "def assign_amenities(row):\n",
    "    carrier = row['carrier']\n",
    "    aircraft = row['equipment_cd_icao']\n",
    "    distance = row['distance_in_km']\n",
    "    \n",
    "    # Determine airline type\n",
    "    if carrier in low_cost_airlines:\n",
    "        return \"Food\"  # Low-cost: only Food\n",
    "    elif carrier in full_service_airlines:\n",
    "        # Full-service airlines\n",
    "        if aircraft in small_aircraft:\n",
    "            return \"Food\"  # Small aircraft: only Food\n",
    "        else:\n",
    "            return \"Food, Entertainment\"  # Full-service: Food + Entertainment\n",
    "    else:\n",
    "        # Default for other airlines (assume full-service for international)\n",
    "        if aircraft in small_aircraft:\n",
    "            return \"Food\"\n",
    "        else:\n",
    "            return \"Food, Entertainment\"\n",
    "\n",
    "# Step 4: Apply the amenities mapping to the dataset\n",
    "df['amenities'] = df.apply(assign_amenities, axis=1)\n",
    "\n",
    "# Step 5: Verify the updated amenities distribution\n",
    "print(\"\\nUpdated Amenities Distribution:\")\n",
    "print(df['amenities'].value_counts(dropna=False))\n",
    "\n",
    "# Step 6: Save the updated dataset\n",
    "OUTPUT_PATH = r\"D:\\work\\WDF_032025_CO2e_with_amenities_otp_updated.csv\"\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"\\nUpdated dataset saved to: {OUTPUT_PATH}\")\n",
    "\n",
    "# Step 7: Show sample data (first 5 rows)\n",
    "print(\"\\nSample data with updated amenities (first 5 rows):\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ba1b7c39-bbb0-4e07-809c-8412d38deff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Amenities Distribution:\n",
      "amenities\n",
      "No Amenities                        7936540\n",
      "Food, Tv, Headphones                   7673\n",
      "Food, Wi-Fi                            5636\n",
      "Food, Wifi, Elec, Tv, Headphones       5511\n",
      "Food                                   5503\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Updated Amenities Distribution:\n",
      "amenities\n",
      "Food, Entertainment    5890819\n",
      "Food                   2070044\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Updated dataset saved to: D:\\work\\WDF_032025_CO2e_with_amenities_otp_updated.csv\n",
      "\n",
      "Sample data with updated amenities (first 5 rows):\n",
      "  carrier  fltno fltsuffx depapt arrapt  distance_in_km equipment_cd_icao  \\\n",
      "0      VT    979      NaN    AAA    FAC        75.63898              AT72   \n",
      "1      VT    959      NaN    AAA    FAC        75.63898              AT72   \n",
      "2      VT    959      NaN    AAA    FAC        75.63898              AT72   \n",
      "3      VT    959      NaN    AAA    FAC        75.63898              AT72   \n",
      "4      VT    979      NaN    AAA    FAC        75.63898              AT72   \n",
      "\n",
      "   seatsfst  seatsbus  seatspremeco  seatseco  stops  co2e_per_passenger_kg  \\\n",
      "0         0         0             0        58      0                  11.44   \n",
      "1         0         4             0        52      0                  12.25   \n",
      "2         0         0             0        56      0                  11.44   \n",
      "3         0         4             0        52      0                  12.25   \n",
      "4         0         4             0        60      0                  12.15   \n",
      "\n",
      "  amenities   otp%  \n",
      "0      Food   50.0  \n",
      "1      Food   40.0  \n",
      "2      Food   75.0  \n",
      "3      Food  100.0  \n",
      "4      Food   60.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the dataset\n",
    "DATASET_PATH = r\"D:\\work\\WDF_032025_CO2e_with_amenities_otp.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(DATASET_PATH, dtype={'fltsuffx': str})\n",
    "\n",
    "# Step 1: Check current amenities distribution\n",
    "print(\"Current Amenities Distribution:\")\n",
    "print(df['amenities'].value_counts(dropna=False))\n",
    "\n",
    "# Step 2: Define comprehensive mapping\n",
    "# IATA-based airline type mapping\n",
    "low_cost_airlines = [\n",
    "    '6E', 'QP', 'SG', 'G8', 'I5',  # Indian low-cost\n",
    "    'AK', 'D7', 'FD', 'FR', 'U2', 'W6', 'NK', 'F9', 'B6', 'WN',  # International low-cost\n",
    "    'VY', 'DY', 'EW', 'JQ', '3K', 'OD', 'IX', 'G9', 'FZ',\n",
    "    # Add more low-cost airlines\n",
    "    'XY',  # Flynas (Saudi Arabia)\n",
    "    'PC',  # Pegasus Airlines (Turkey)\n",
    "    'W4',  # Wizz Air Malta\n",
    "    '3O',  # Air Arabia Maroc\n",
    "    '5W',  # Wizz Air Abu Dhabi\n",
    "    'BL',  # Jetstar Pacific (Vietnam)\n",
    "    'Z2',  # AirAsia Philippines\n",
    "    'SL',  # Thai Lion Air\n",
    "    'ID',  # Batik Air (semi-low-cost)\n",
    "    'VA',  # Virgin Australia (semi-low-cost)\n",
    "    'WS',  # WestJet (semi-low-cost)\n",
    "]\n",
    "\n",
    "full_service_airlines = [\n",
    "    'AI', 'VT', 'UK', '9I',  # Indian full-service\n",
    "    'EK', 'SQ', 'QR', 'LH', 'BA', 'AF', 'KL', 'DL', 'AA', 'UA',  # International full-service\n",
    "    'CX', 'JL', 'NH', 'KE', 'OZ', 'TG', 'MH', 'GA', 'VN', 'PR',\n",
    "    'QF', 'NZ', 'ET', 'TK', 'MS', 'SV', 'EY', 'KU', 'WY', 'UL',\n",
    "    'IB', 'AY', 'LO', 'OS', 'LX', 'SK', 'TP', 'AC', 'AV', 'LA',\n",
    "    'AM', 'AR', 'CA', 'CZ', 'MU', 'HU'\n",
    "]\n",
    "\n",
    "# ICAO-based aircraft type mapping\n",
    "small_aircraft = [\n",
    "    # Turboprops\n",
    "    'AT72', 'AT43', 'DH8A', 'DH8B', 'DH8C', 'DH8D', 'DHC2', 'DHC4', 'DHC6', 'DHC7',\n",
    "    'BN2P', 'SF34', 'JS31', 'JS32', 'JS41', 'L410', 'SW4',\n",
    "    # Regional Jets\n",
    "    'CRJ1', 'CRJ2', 'CRJ7', 'CRJ9', 'CRJX', 'E110', 'E120', 'E135', 'E145', 'E170',\n",
    "    'B190', 'D228', 'D328', 'J328', 'RJ85',\n",
    "    # Other Small Aircraft\n",
    "    'C208', 'PC12', 'TRIS', 'AN24', 'AN26', 'AN38', 'YK40', 'YK42',\n",
    "    # Helicopters\n",
    "    'A109', 'A139', 'B06', 'EC30', 'EC55', 'S76', 'MI8'\n",
    "]\n",
    "\n",
    "# Step 3: Define amenities assignment logic\n",
    "def assign_amenities(row):\n",
    "    carrier = row['carrier']\n",
    "    aircraft = row['equipment_cd_icao']\n",
    "    distance = row['distance_in_km']\n",
    "    \n",
    "    # Determine airline type\n",
    "    if carrier in low_cost_airlines:\n",
    "        return \"Food\"  # Low-cost: only Food\n",
    "    elif carrier in full_service_airlines:\n",
    "        # Full-service airlines\n",
    "        if aircraft in small_aircraft:\n",
    "            return \"Food\"  # Small aircraft: only Food\n",
    "        else:\n",
    "            return \"Food, Entertainment\"  # Full-service: Food + Entertainment\n",
    "    else:\n",
    "        # Default for other airlines (assume full-service for international)\n",
    "        if aircraft in small_aircraft:\n",
    "            return \"Food\"\n",
    "        else:\n",
    "            return \"Food, Entertainment\"\n",
    "\n",
    "# Step 4: Apply the amenities mapping to the dataset\n",
    "df['amenities'] = df.apply(assign_amenities, axis=1)\n",
    "\n",
    "# Step 5: Verify the updated amenities distribution\n",
    "print(\"\\nUpdated Amenities Distribution:\")\n",
    "print(df['amenities'].value_counts(dropna=False))\n",
    "\n",
    "# Step 6: Save the updated dataset\n",
    "OUTPUT_PATH = r\"D:\\work\\WDF_032025_CO2e_with_amenities_otp_updated.csv\"\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"\\nUpdated dataset saved to: {OUTPUT_PATH}\")\n",
    "\n",
    "# Step 7: Show sample data (first 5 rows)\n",
    "print(\"\\nSample data with updated amenities (first 5 rows):\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "425be9d5-b933-4036-8522-153a1bdba7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://10.212.134.10:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with watchdog (windowsapi)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Path to the updated dataset\n",
    "DATASET_PATH = r\"C:\\Users\\amit.murmu\\Desktop\\flight_emissions_demo\\WDF_032025_CO2e_with_amenities_otp_updated.csv\"\n",
    "\n",
    "# Load and cache the dataset with dtype specification\n",
    "df = pd.read_csv(DATASET_PATH, dtype={'fltsuffx': str}, low_memory=False)\n",
    "\n",
    "# Get unique airports for autocomplete\n",
    "airports = sorted(set(df['depapt'].tolist() + df['arrapt'].tolist()))\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html', airports=airports)\n",
    "\n",
    "@app.route('/search', methods=['POST'])\n",
    "def search():\n",
    "    # Get form inputs\n",
    "    depapt = request.form.get('depapt', '').strip().upper()\n",
    "    arrapt = request.form.get('arrapt', '').strip().upper()\n",
    "    fltno = request.form.get('fltno', '').strip()\n",
    "    \n",
    "    # Validate inputs\n",
    "    if not depapt or not arrapt:\n",
    "        return jsonify({'error': 'Please enter both Departure and Arrival airports.'})\n",
    "    \n",
    "    # Filter the dataset\n",
    "    filtered_df = df[\n",
    "        (df['depapt'] == depapt) & \n",
    "        (df['arrapt'] == arrapt)\n",
    "    ]\n",
    "    \n",
    "    # If flight number is provided, filter further\n",
    "    if fltno:\n",
    "        filtered_df = filtered_df[filtered_df['fltno'].astype(str) == fltno]\n",
    "    \n",
    "    # If no results found\n",
    "    if filtered_df.empty:\n",
    "        return jsonify({'error': 'No flights found for the given criteria.'})\n",
    "    \n",
    "    # Select relevant columns\n",
    "    filtered_df = filtered_df[[\n",
    "        'carrier', 'fltno', 'depapt', 'arrapt', \n",
    "        'distance_in_km', 'co2e_per_passenger_kg', \n",
    "        'amenities', 'otp%'\n",
    "    ]]\n",
    "    \n",
    "    # Convert to list of dictionaries\n",
    "    data = filtered_df.to_dict('records')\n",
    "    return jsonify({'data': data})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, host='0.0.0.0', port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf9a3b6-6bbd-4523-bdf0-ccf7ec882f40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
